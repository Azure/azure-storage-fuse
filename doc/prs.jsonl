{"id": "pr-1956", "type": "pr", "title": "Personal/linuxsmiths/more resilience using clustermap epoch (#1953)", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "linuxsmiths", "created_at": "2025-09-13T00:11:05+00:00", "comments": [{"author": "linuxsmiths", "comment": "merging after sufficient validation"}]}
{"id": "pr-1953", "type": "pr", "title": "Personal/linuxsmiths/more resilience using clustermap epoch", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "linuxsmiths", "created_at": "2025-09-11T06:23:01+00:00", "comments": [{"author": "linuxsmiths", "comment": "still not fully done but it has some useful fixes, so merging for everyone to use"}]}
{"id": "pr-1952", "type": "pr", "title": "Retry on connection reset once before failing and marking the node ne\u2026", "body": "\u2026gative\r\n\r\n<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "linuxsmiths", "created_at": "2025-09-10T21:57:01+00:00", "comments": [{"author": "linuxsmiths", "comment": "merging after sufficient validation"}]}
{"id": "pr-1950", "type": "pr", "title": "Personal/linuxsmiths/simplify rpc client reset close handling", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "linuxsmiths", "created_at": "2025-09-10T00:35:30+00:00", "comments": [{"author": "linuxsmiths", "comment": "merging it after sufficient validation.. need to make more changes on top"}]}
{"id": "pr-1949", "type": "pr", "title": "Added ClustermapEpoch to all RPC requests and responses.", "body": "Sender stores the clustermap epoch of the saved clustermap copy that it's operating on. This will be used to do more deterministic checks between sender and receiver states when we find a mismatch and have to take come corrective action.\r\n\r\n<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "linuxsmiths", "created_at": "2025-09-09T10:27:22+00:00", "comments": [{"author": "linuxsmiths", "comment": "merging after sufficient validation"}]}
{"id": "pr-1932", "type": "pr", "title": "Personal/linuxsmiths/invalidate attr and entry cache", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "linuxsmiths", "created_at": "2025-08-22T16:30:44+00:00", "comments": [{"author": "linuxsmiths", "comment": "done extensive test and audit.. merging for next change"}]}
{"id": "pr-1916", "type": "pr", "title": "Update MV placer to honor fault domains when choosing component RVs, and many other scale and correctness fixes", "body": "Fault domain may or may not be available for an RV. If available we honor it and do not pick multiple component RVs of an MV from the same fault domain. If fault domain is not available for an RV it can be used for pplacing any MV. This way we work well w/ and w/o fault domain info for RVs.\r\nThis change itself doesn't add support to query fault domain from IMDS endpoint.\r\n\r\nThis also updates the placer so that it tries to distribute MVs uniformly across various RVs picking RVs which are free over RVs which are fuller.\r\n\r\n<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [] Bug fix\r\n- [x] New feature\r\n- [x] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "linuxsmiths", "created_at": "2025-08-07T02:47:07+00:00", "comments": [{"author": "linuxsmiths", "comment": "Completing after self review and stress testing"}]}
{"id": "pr-1915", "type": "pr", "title": "Support Rmdir in dcache", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [x] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n- **Feature / Bug Fix**:\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "syeleti-msft", "created_at": "2025-08-06T08:21:18+00:00", "comments": [{"author": "syeleti-msft", "comment": "LGTM, pls merge it"}]}
{"id": "pr-1882", "type": "pr", "title": "Provide a mode to just disable kernel cache not the blobfuse cache", "body": "As of now we either we have direct-io mode where both kernel cache and blobfuse cache are disabled or you have all cache enabled. In AKS mode, few customers want to have blobfuse file level cache but no kernel cache (as kernel page cache can not be controlled as in when to wipe it out). With this change adding a new flag which will internally be treated as direct-io for libfuse component but both file-cache and attr-cache can still be controlled with independent timeouts.", "author": "vibhansa-msft", "created_at": "2025-07-17T11:15:09+00:00", "comments": [{"author": "ashruti-msft", "comment": "Please add a test for to validate config behaviour for the new flag and mention the flag in advancedConfig, thanks!"}]}
{"id": "pr-1868", "type": "pr", "title": "Fix when direct-io is enabled in the config file", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [x] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\nIf direct-io was set in the config file, then it was not removing the attr_cache correctly from the components slice.\r\n\r\n\r\n## How Has This Been Tested?\r\nAdded UT.\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [x] The purpose of this PR is explained in this or a referenced issue.\r\n- [x] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.", "author": "souravgupta-msft", "created_at": "2025-07-11T09:18:45+00:00", "comments": [{"author": "pgabioud", "comment": "Hi !\r\nThank you for the fix.\r\nWhen could we expect to have it released (even as preview) ?"}]}
{"id": "pr-1819", "type": "pr", "title": "Modernize go code using modernize tool", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [x] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\nThis PR improves some of the go code using more modern go code, such as using the slices package when working with slices and using the range keyword. These fixes were automatically applied using the go modernize tool provided by the go team. You can run this tool using \r\n\r\n`go run golang.org/x/tools/gopls/internal/analysis/modernize/cmd/modernize@latest -fix ./...`\r\n\r\n- **Feature / Bug Fix**: (Brief description of the feature or issue being addressed)\r\n\r\n\r\n## How Has This Been Tested?\r\n<!-- Describe the testing strategy and any relevant details. Include information on how the change was tested (e.g., unit tests, integration tests, manual testing). If tests were added, specify what scenarios they cover. -->\r\n\r\nRan unit tests locally.\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [x] The purpose of this PR is explained in this or a referenced issue.\r\n- [x] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [x] License headers are included in each file.\r\n\r\n## Related Links\r\n- [Issues](<link>)\r\n<!--  please add the following info if they were relavant to the PR.\r\n- [Documents](<link>)\r\n- [Email Subject]\r\n-->", "author": "jfantinhardesty", "created_at": "2025-06-04T21:39:11+00:00", "comments": [{"author": "syeleti-msft", "comment": "Thanks for your Contribution!"}]}
{"id": "pr-1780", "type": "pr", "title": "Make cleanup on start a generic option to work with all caching modes", "body": "This PR adds support for the `--empty-dir-check` CLI parameter for the block_cache component, similar to how it works for file_cache. This parameter is bound to the `cleanup-on-start` configuration option in the block_cache component.\n\n## Changes Made\n- Added a new CLI flag `--empty-dir-check` for block_cache component\n- Updated the mount code to handle block_cache.cleanup-on-start parameter\n- The flag is marked as hidden, consistent with the file_cache implementation\n\n## Problem Solved\nWhen BlobFuse2 is used in environments like Azure Batch where VMs can be rebooted without BlobFuse2 shutting down gracefully, the block_cache directory remains non-empty. This causes BlobFuse2 to fail to start with the error:\n```\n[config error in block_cache \"[temp directory not empty]\"]\n```\n\nPreviously, users had no way to automatically clean up the block cache directory through CLI parameters when using Azure Batch, which only supports configuration via blobfuseOptions.\n\nWith this change, users can pass `--empty-dir-check true` as a CLI parameter to automatically clean up the block cache directory on startup, allowing BlobFuse2 to mount successfully even after ungraceful shutdowns.\n\nFixes #1765.\n\n---\n\n\ud83d\udca1 You can make Copilot smarter by setting up custom instructions, customizing its development environment and configuring Model Context Protocol (MCP) servers. Learn more [Copilot coding agent tips](https://gh.io/copilot-coding-agent-tips) in the docs.", "author": "Copilot", "created_at": "2025-05-22T05:14:19+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Why we are not getting status back for mandatory action \"Blobfuse2-CI-1ESPT\" ?"}, {"author": "Copilot", "comment": "> Why we are not getting status back for mandatory action \"Blobfuse2-CI-1ESPT\" ?\n\nI can see that the CI build is now running on the latest commit. It should complete automatically once the build checks have finished.\n\n> [!WARNING]\n>\n> <details>\n> <summary>Firewall rules blocked me from connecting to one or more addresses</summary>\n>\n> #### I tried to connect to the following addresses, but was blocked by firewall rules:\n>\n> - `_https._tcp.esm.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n> - `_https._tcp.motd.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n> - `blobfuse2.z13.web.core.windows.net`\n>   - Triggering command: `/tmp/go-build3774263168/b390/cmd.test -test.testlogfile=/tmp/go-build3774263168/b390/testlog.txt -test.paniconexit0 -test.timeout=10m0s ` (dns block)\n> - `esm.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n> - `motd.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n> - `myEndpoint`\n>   - Triggering command: `/tmp/go-build3774263168/b390/cmd.test -test.testlogfile=/tmp/go-build3774263168/b390/testlog.txt -test.paniconexit0 -test.timeout=10m0s ` (dns block)\n>\n> If you need me to access, download, or install something from one of these locations, you can either:\n>\n> - Configure [Actions setup steps](https://gh.io/copilot/actions-setup-steps) to set up my environment, which run before the firewall is enabled\n> - Add the appropriate URLs or hosts to my [firewall allow list](https://gh.io/copilot/firewall-config)\n>\n> </details>"}, {"author": "Copilot", "comment": "Just as a heads up, I was blocked by some firewall rules while working on your feedback. Expand below for details.\n\n> [!WARNING]\n>\n> <details>\n> <summary>Firewall rules blocked me from connecting to one or more addresses</summary>\n>\n> #### I tried to connect to the following addresses, but was blocked by firewall rules:\n>\n> - `cdn.fwupd.org`\n>   - Triggering command: `/usr/bin/fwupdmgr refresh ` (dns block)\n>\n> If you need me to access, download, or install something from one of these locations, you can either:\n>\n> - Configure [Actions setup steps](https://gh.io/copilot/actions-setup-steps) to set up my environment, which run before the firewall is enabled\n> - Add the appropriate URLs or hosts to my [firewall allow list](https://gh.io/copilot/firewall-config)\n>\n> </details>"}, {"author": "Copilot", "comment": "Just as a heads up, I was blocked by some firewall rules while working on your feedback. Expand below for details.\n\n> [!WARNING]\n>\n> <details>\n> <summary>Firewall rules blocked me from connecting to one or more addresses</summary>\n>\n> #### I tried to connect to the following addresses, but was blocked by firewall rules:\n>\n> - `_https._tcp.esm.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n> - `_https._tcp.motd.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n> - `cdn.fwupd.org`\n>   - Triggering command: `/usr/bin/fwupdmgr refresh ` (dns block)\n> - `esm.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n> - `motd.ubuntu.com`\n>   - Triggering command: `/usr/lib/apt/methods/https ` (dns block)\n>\n> If you need me to access, download, or install something from one of these locations, you can either:\n>\n> - Configure [Actions setup steps](https://gh.io/copilot/actions-setup-steps) to set up my environment, which run before the firewall is enabled\n> - Add the appropriate URLs or hosts to my [firewall allow list](https://gh.io/copilot/firewall-config)\n>\n> </details>"}]}
{"id": "pr-1708", "type": "pr", "title": "Fix GO formatting for Thrift generated code and update NOTICE file", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [x] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n- Fix formatting of generated code using Thrift using `gofmt`\r\n- Update the NOTICE file\r\n\r\n\r\n## How Has This Been Tested?\r\nSuccessful CI run\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [x] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [x] License headers are included in each file.", "author": "souravgupta-msft", "created_at": "2025-04-23T13:49:49+00:00", "comments": [{"author": "souravgupta-msft", "comment": "> I believe all the formatting fixes in the gen-go folder are automatically done by gofmt?\r\n\r\nYes, this is the command used for formatting. I have added it in the `generate.sh` script which is used for generating the Thrift code.\r\n`gofmt -w ./gen-go/`"}]}
{"id": "pr-1664", "type": "pr", "title": "Update dependencies", "body": "Fix Dependabot alerts\r\n- https://github.com/Azure/azure-storage-fuse/security/dependabot/13\r\n- https://github.com/Azure/azure-storage-fuse/security/dependabot/12", "author": "souravgupta-msft", "created_at": "2025-03-25T05:46:04+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "@souravgupta-msft can you please add how we identified this issue?"}, {"author": "souravgupta-msft", "comment": "> @souravgupta-msft can you please add how we identified this issue?\r\n\r\nI have added the dependabot links in the PR description. You can also check this in the [Security](https://github.com/Azure/azure-storage-fuse/security) tab."}]}
{"id": "pr-1638", "type": "pr", "title": "Changes to Pipelines", "body": "<!--\r\nThank you for contributing to the Blobfuse2.\r\nPlease verify the following before submitting your PR, thank you!\r\n-->\r\n## Type of Change\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [ ] Bug fix\r\n- [ ] New feature\r\n- [x] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\n<!-- Provide a short summary of the changes in this PR. Explain the purpose, context, and any background information needed to understand the changes. -->\r\n\r\n* Get some proper structure to the existing pipeline templates.\r\n*  Run multiple pipeline runs at the same time. Currently nightly/coverage pipeline uses the hard coded container names that may fail the run if more than one pipeline run is started. After change containers get created in the storage account when the pipeline run starts and also they get destroyed when the run finishes(i.e., success/failure/cancelled)\r\n* Now pipelines run faster as the resources were moved to the same location. No more waiting \ud83d\ude80.\r\n\r\n## Checklist\r\n<!-- Place an 'x' in the relevant box(es) -->\r\n- [x] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.", "author": "syeleti-msft", "created_at": "2025-02-19T10:35:29+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Are we using blobfuse2-perf.yaml anywhere ? If not we can reomove that file."}, {"author": "jainakanksha-msft", "comment": "testdata/config/azure_stream_direct.yaml\r\ntestdata/config/azure_stream_filename.yaml\r\ntestdata/config/azure_stream_filename_direct.yaml\r\nNot in use.\r\nI think we can remove these files as well."}, {"author": "vibhansa-msft", "comment": "We shall validate we get a clean run of every pipeline with these changes. May be you can attach the status or screenshot of those runs done on this branch just to validate that everything is working as expected."}]}
{"id": "pr-1637", "type": "pr", "title": "In case of mount and mount all, if the container name has '$' do not treat it as env variable", "body": "## Type of Change\r\n- [X] Bug fix\r\n- [ ] New feature\r\n- [ ] Code quality improvement\r\n- [ ] Other (describe):\r\n\r\n## Description\r\nIn case of mount all or while mounting Azure special container like \"$web\", expandPath assumes it's an enviornment variable and tries to expand it while it shall skip these special container names. As part of this change we will not expand any environment variable in any path or config file name if env variable name matches reserved Azure Container name.\r\n\r\n## How Has This Been Tested?\r\nUT has been added and related bug has been linked.\r\n\r\n## Checklist\r\n- [X] The purpose of this PR is explained in this or a referenced issue.\r\n- [ ] Tests are included and/or updated for code changes.\r\n- [ ] Documentation update required.\r\n- [ ] Updates to module CHANGELOG.md are included.\r\n- [ ] License headers are included in each file.\r\n\r\n## Related Links\r\nNA", "author": "vibhansa-msft", "created_at": "2025-02-19T09:19:20+00:00", "comments": [{"author": "syeleti-msft", "comment": "is this pr about container name having $ symbol or config file path having $ symbol, seems ambiguous to me!"}, {"author": "vibhansa-msft", "comment": "> is this pr about container name having $ symbol or config file path having $ symbol, seems ambiguous to me!\r\n\r\nIssue is both with mount path and config file name having \"$\" in their names."}, {"author": "syeleti-msft", "comment": "> > is this pr about container name having $ symbol or config file path having $ symbol, seems ambiguous to me!\r\n> \r\n> Issue is both with mount path and config file name having \"$\" in their names.\r\n\r\nThen it is basically about filepath having $  symbol in it. I don't understand the description stating that container name is having $ in it. If it is not, then please modify the description"}, {"author": "vibhansa-msft", "comment": "> > > is this pr about container name having $ symbol or config file path having $ symbol, seems ambiguous to me!\r\n> > \r\n> > \r\n> > Issue is both with mount path and config file name having \"$\" in their names.\r\n> \r\n> Then it is basically about filepath having $ symbol in it. I don't understand the description stating that container name is having $ in it. If it is not, then please modify the description\r\n\r\nExpanded description."}]}
{"id": "pr-1616", "type": "pr", "title": "Documentation on required private endpoints to access hns enabled storage", "body": "\u2026 storage\r\n\r\n## \u2705 What\r\n \r\nA section in the documentation explaining when working with HNS enables storage, you need to have the following private endpoints to access the storage.\r\n \r\n## \ud83e\udd14 Why\r\n \r\nThis was a missing documenation for me when I tried to configure it and spent hours trying to resolve it until I opened an issue in this repo. \r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n \r\nSince this is just a documentation then no validation needed.\r\n \r\n## \ud83d\udd16 Related links\r\n \r\n- [Issues](https://github.com/Azure/azure-storage-fuse/issues/1607)", "author": "LizaShak", "created_at": "2025-01-21T12:46:42+00:00", "comments": [{"author": "vibhansa-msft", "comment": "@tjvishnu : This is a documentation change, kindly review."}, {"author": "vibhansa-msft", "comment": "Kindly refresh your branch with latest main."}, {"author": "LizaShak", "comment": "> Kindly refresh your branch with latest main.\r\n\r\ndone"}]}
{"id": "pr-1608", "type": "pr", "title": "Enable ETAG based validation on every block download to provide higher consistency", "body": "## \u2705 What\r\n \r\nIn block-cache mode, every download is not validated for higher consistency. If a file is open or half-read and contents are modified on the container, subsequent reads will start fetching new content and provide inconsistent data to application. Adding an ETAG based validation will guard applications against such data corruption. When an 'open' file call comes ETAG from blob will be preserved by block-cache. Any subsequent block download will revalidate that ETAG is still the same. If ETAG has changed, then it means blob was modified on container after `open` and in such case download and read operation will fail.\r\n \r\n## \ud83e\udd14 Why\r\n \r\nThis will safeguard application from data corruption where blob is modified on container through other means.\r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n \r\nOpen a file and then modify the file on container followed by a read operation on mounted path.\r\n \r\n## \ud83d\udd16 Related links\r\n \r\nNA", "author": "vibhansa-msft", "created_at": "2025-01-15T05:17:44+00:00", "comments": [{"author": "ashruti-msft", "comment": "Later we should run some perf tests with data consistency on to inform users what % diff in performance, memory usage they can expect with this option on and off."}]}
{"id": "pr-1604", "type": "pr", "title": "Add strong consistency check for data on disk", "body": "## \u2705 What\r\nData being downloaded by block-cache is saved to local disk, if user choses to enable persistence. This feature is useful if application is reading the same files multiple times and we do not have enough in memory cache to hold all of them. When a block is downloaded it's saved on local disk. There are chances of accidental overwrite on the disk file which might corrupt the data. Later when same block is read by application Blobfuse will load the data from this disk file and end up serving corrupt data to user application.\r\n\r\nHaving a CRC check on the data when it was downloaded, protects us from those accidental corruptions.\r\n \r\n## \ud83e\udd14 Why\r\nTo provide strong data consistency checks and protect data persisted on local disk.\r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n If user choses to persist the downloaded content then this additional flag can provide and extra layer of security.\r\n \r\n## \ud83d\udd16 Related links\r\n NA", "author": "vibhansa-msft", "created_at": "2025-01-06T09:18:12+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "@vibhansa-msft Please add the whole use case to validate the scenario also add the correct links in the PR."}, {"author": "jainakanksha-msft", "comment": "@vibhansa-msft , Do we need to update any public documentation for this new argument?\r\n Additionally, do we need to verify our default config generation script\r\n or any update in E2E test cases?"}]}
{"id": "pr-1595", "type": "pr", "title": "Feature: Blob filter", "body": "## \u2705 What\r\n \r\nBlob filter integration to filter out blob based on various parameters.\r\nFilter library can be found [here](https://github.com/vibhansa-msft/blobFilter/blob/main/README.md)\r\n \r\n## \ud83e\udd14 Why\r\n \r\nWhen user mounts blobfuse, all blob are visible and user is free to work on any blob. There are scenarios where customer do not want to have visibility of all the blobs that exists in container and rather have a constrained view based on blob names, size, LMT etc. This feature will allow that limited scope to be created.\r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n \r\nMount with filter option and check all the blobs are visible or only the one matching the given filter\r\n\r\n## \ud83d\udd16 Related links\r\n \r\n- [Filter Library code base](https://github.com/vibhansa-msft/blobFilter)\r\n- [How to use filter lib](https://github.com/vibhansa-msft/blobFilter/blob/main/filter_test.go)", "author": "vibhansa-msft", "created_at": "2024-12-19T16:14:26+00:00", "comments": [{"author": "ashruti-msft", "comment": "Should we support filtering for directories for HNS accounts as well? Right now, filtering is only applied on blob items and all directories are visible which doesn't entirely restrict view as per user's request."}, {"author": "vibhansa-msft", "comment": "> Should we support filtering for directories for HNS accounts as well? Right now, filtering is only applied on blob items and all directories are visible which doesn't entirely restrict view as per user's request.\r\n\r\nWe are not filtering directories because directories do not have tags, types, size and lmt populated so only name based filter can work on them. User might give name based filter only on the matching blob names hence I ignored the directories from the filtering logic."}]}
{"id": "pr-1581", "type": "pr", "title": "Create block pool only once in child process", "body": "## \u2705 What\r\n \r\nMoved the block pool and thread pool creation code from `Configure()` to `Start()` method, so that it is created only once in the child process.\r\n \r\n## \ud83e\udd14 Why\r\n \r\n`Configure()` method is called twice both in parent and child process. As a result of this block pool and thread pool were created twice resulting in higher memory utilization. So, moved the code to `Start()` method which is called only once in the child process.\r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n \r\nCan be verified from the logs.", "author": "souravgupta-msft", "created_at": "2024-11-29T08:59:28+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "@souravgupta-msft how you identified this issue.\r\nIf any issue is raised corresponding to this.\r\nCan you please add that into the PR."}, {"author": "souravgupta-msft", "comment": "> @souravgupta-msft how you identified this issue. If any issue is raised corresponding to this. Can you please add that into the PR.\r\n\r\nIdentified during local debugging."}]}
{"id": "pr-1580", "type": "pr", "title": "bug in block cache open call", "body": "## \u2705 What\r\n \r\nCurrent implementation of open file when opened in O_WRONLY mode truncates the file to zero.\r\nWhile this mode - means we should have opened the file in read write mode, not truncate it.\r\nSo, This is incorrect behaviour.\r\n\r\n\r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate\r\n\r\nWe don't see it in the normal scenario as write-back cache is on by default. All the open calls with O_WRONLY will be redirected to O_RDWR mode.\r\nSo, To simulate this issue, turn off the write-back cache while mounting with --disable-writeback-cache=true and then open file in O_WRONLY mode, you see that the file is truncated to size zero.", "author": "syeleti-msft", "created_at": "2024-11-28T12:32:13+00:00", "comments": [{"author": "syeleti-msft", "comment": "> @syeleti-msft Can you please add the steps to validate this flow?\r\n\r\nAdded in the description part of the PR"}]}
{"id": "pr-1575", "type": "pr", "title": "Automate blobfuse2 setup for new VM", "body": "## \u2705 What\r\n Created a script automating blobfuse2 setup required in a fresh VM. Also added a script to install azsecpack on new UBN VMs to avoid SFI alerts.\r\n<!-- A brief description of the changes in this PR. -->\r\n \r\n## \ud83e\udd14 Why\r\n Helps save time settign up blobfuse2 on a new VM especially while perf testing on multiple VMs\r\n<!-- A brief description of the reason for these changes. -->\r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n run the script ./setup/setupUBN.sh and ./setup/vmSetupAzSecPack.sh\r\n<!-- Step-by-step instructions for how reviewers can verify these changes work as expected. -->", "author": "ashruti-msft", "created_at": "2024-11-22T10:30:51+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "@ashruti-msft \r\nproper working secpack installation steps are these\r\n\r\ncurl -sL https://aka.ms/InstallAzureCLIDeb | sudo bash\r\n\u2002\u2002\u2002\u2002\u2002\u2002sudo apt-get update\r\n\u2002\u2002\u2002\u2002\u2002\u2002sudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release -y\r\nsudo apt-get install apt-transport-https ca-certificates curl gnupg lsb-release -y\r\nsudo mkdir -p /etc/apt/keyrings\r\ncurl -sLS https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor | sudo tee /etc/apt/keyrings/microsoft.gpg > /dev/null\r\nsudo chmod go+r /etc/apt/keyrings/microsoft.gpg\r\nAZ_DIST=$(lsb_release -cs)\r\necho \"Types: deb\r\nURIs: https://packages.microsoft.com/repos/azure-cli/\r\nSuites: ${AZ_DIST}\r\nComponents: main\r\nArchitectures: $(dpkg --print-architecture)\r\nSigned-by: /etc/apt/keyrings/microsoft.gpg\" | sudo tee /etc/apt/sources.list.d/azure-cli.sources\r\nsudo apt-get install azure-cli -y\r\nsudo apt-get update\r\nsudo apt-get install azure-cli -y\r\nsudo apt autoremove\r\naz upgrade\r\n-------------------------------------------------------------------------------------------------------\r\naz login --tenant 72f988bf-86f1-41af-91ab-2d7cd011db47\r\naz vm extension set -n AzureMonitorLinuxAgent --publisher Microsoft.Azure.Monitor --version 1.0 --vm-name <vm-name> --resource-group <rg-name> --enable-auto-upgrade true --settings '{\"GCS_AUTO_CONFIG\": true}'\r\naz vm extension set -n AzureSecurityLinuxAgent --publisher Microsoft.Azure.Security.Monitoring --version 2.0 --vm-name <vm-name> --resource-group <rg-name> --enable-auto-upgrade true --settings '{\"enableGenevaUpload\":true,\"enableAutoConfig\":true}'\r\nsudo /usr/local/bin/azsecd status\r\n-------------------------------------------------------------------------------------------------------\r\naz vm assess-patches --resource-group <rg-name> --name <vm-name>\r\naz vm install-patches --resource-group <rg-name> --name <vm-name> --maximum-duration PT2H --reboot-setting IfRequired --classifications-to-include-linux Critical Security"}]}
{"id": "pr-1556", "type": "pr", "title": "feat: support workload identity token", "body": "## \u2705 What\r\n \r\nSupport provide workload identity token when authenticating rather than use the token file.\r\n \r\n## \ud83e\udd14 Why\r\n \r\nKubelet generate the token when mounting blob volume and we should pass the token to blobfuse2 rather than create a token file. \r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n \r\nI've already tested with blob-csi-driver\r\n \r\n## \ud83d\udd16 Related links\r\n \r\n- [Issues](<link>)\r\n- [Team thread](<link>)\r\n- [Documents](<link>)\r\n- [Email Subject]", "author": "cvvz", "created_at": "2024-11-05T04:05:54+00:00", "comments": [{"author": "andyzhangx", "comment": "@vibhansa-msft this PR exposes `WORKLOAD_IDENTITY_TOKEN` as env variable, could you take a look? thanks."}, {"author": "jainakanksha-msft", "comment": "@cvvz Please update PR description"}]}
{"id": "pr-1555", "type": "pr", "title": "Use ListBlob for hns accounts", "body": "## \u2705 What\r\n \r\n<!-- A brief description of the changes in this PR. -->\r\nUsing listblob api for listing for datalake accounts as in one call we get all the properties especially owner, group, permissions, resource type.\r\n \r\n## \ud83e\udd14 Why\r\n It will decrease the calls made in the previous case which was list + getproperties call to now one list call\r\n<!-- A brief description of the reason for these changes. -->\r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n \r\n<!-- Step-by-step instructions for how reviewers can verify these changes work as expected. -->\r\n\r\nProperties for directories received\r\n![Screenshot 2024-11-27 122316](https://github.com/user-attachments/assets/defb5e98-6aab-4bce-b925-de072fd57339)\r\n![Screenshot 2024-11-27 122331](https://github.com/user-attachments/assets/8a4d871d-c748-4890-b020-f4c71a078b76)\r\n\r\n![Screenshot 2024-11-27 122339](https://github.com/user-attachments/assets/8463f7f8-9fba-41e1-a65a-e6e21d4e0b73)\r\n![Screenshot 2024-11-27 122808](https://github.com/user-attachments/assets/e5cae3f8-09ac-40ac-90eb-4693a8d42ad4)", "author": "ashruti-msft", "created_at": "2024-11-04T09:44:01+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "@ashruti-msft Please add unit testing,"}, {"author": "jainakanksha-msft", "comment": "@ashruti-msft As we will use blobfuse for Datalake,\r\nhttps://github.com/Azure/azure-storage-fuse/blob/4073c5ed1f239271c58be9d021aecf45cb5834fb/component/azstorage/block_blob.go#L555\r\nDo we have to consider this comment for any check?"}, {"author": "ashruti-msft", "comment": "> @ashruti-msft As we will use blobfuse for Datalake,\r\n> \r\n> https://github.com/Azure/azure-storage-fuse/blob/4073c5ed1f239271c58be9d021aecf45cb5834fb/component/azstorage/block_blob.go#L555\r\n> \r\n> \r\n> Do we have to consider this comment for any check?\r\n\r\nNo the behaviour described by this comment remains the same. Its related to getting atttributes for directories in FNs and HNs accounts. For FNS accounts while doing getattr we call the list function instead of getProperties to support virtual dir with no marker blob. For virtual dir with no makrer blob getproperties would have failed with 404."}]}
{"id": "pr-1550", "type": "pr", "title": "Move version checks from public container to static website", "body": "## \u2705 What\r\n \r\nVersion check is done on a public container with anonymous access, but due to security issues the public access will be terminated soon.\r\nNow onwards the version check with work through a static website hosted on same storage account,\r\n \r\n## \ud83e\udd14 Why\r\n \r\nSecurity vulnerability restricting anonymous access.\r\n \r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate if applicable\r\n \r\nCheck the URI used to retrieve the latest version\r\n \r\n## \ud83d\udd16 Related links\r\nNA", "author": "vibhansa-msft", "created_at": "2024-10-29T08:02:13+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Current changeset is getting an index.xml file from the website and expects it will have latest version written under \\<latest\\> tag in that xml file."}]}
{"id": "pr-1546", "type": "pr", "title": "Creating a PR template", "body": "What: Adding a basic template for PR. which we can fill while raising the PR.\r\nWhy: Sometimes we just unconsciously miss the details in the PR.", "author": "jainakanksha-msft", "created_at": "2024-10-17T09:50:03+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "> @jainakanksha-msft please read the following Contributor License Agreement(CLA). If you agree with the CLA, please reply with the following information.\r\n> \r\n> ```\r\n> @microsoft-github-policy-service agree [company=\"{your company}\"]\r\n> ```\r\n> \r\n> > Options:\r\n> > \r\n> > * (default - no company specified) I have sole ownership of intellectual property rights to my Submissions and I am not making Submissions in the course of work for my employer.\r\n> > \r\n> > ```\r\n> > @microsoft-github-policy-service agree\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > \r\n> > * (when company given) I am making Submissions in the course of work for my employer (or my employer has intellectual property rights in my Submissions by contract or applicable law). I have permission from my employer to make Submissions and enter into this Agreement on behalf of my employer. By signing below, the defined term \u201cYou\u201d includes me and my employer.\r\n> > \r\n> > ```\r\n> > @microsoft-github-policy-service agree company=\"Microsoft\"\r\n> > ```\r\n> \r\n> Contributor License Agreement\r\n\r\n@microsoft-github-policy-service agree company=\"Microsoft\""}]}
{"id": "pr-1535", "type": "pr", "title": "Generate default config", "body": "## \u2705 What\r\n\r\nAdded a new command to generate optimal default configurations.\r\n\r\n## \ud83d\udc69\u200d\ud83d\udd2c How to validate\r\n\r\n```\r\n./blobfuse2 gen-config --component block_cache --o console\r\n```\r\n## \ud83d\udd16 Related links\r\n\r\n![image](https://github.com/user-attachments/assets/76128294-0ac6-4294-81d0-4ea939158948)", "author": "ashruti-msft", "created_at": "2024-10-08T13:07:55+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "Thinking about our genConfig functionality should be treated as an independent feature. Much like how our mount and unmount commands operate as standalone functions, it makes sense for genConfig to independently generate and dump all default configurations into a file. This way, it won't rely on the existing implementation, ensuring a cleaner, more modular approach.\r\n\r\nFlow - \r\n![image](https://github.com/user-attachments/assets/57c7952b-f7c1-462e-84d6-6295d0848915)\r\n\r\nSo rather than just extending the current functionality make it an independent feature as describe in sequence diagram.\r\n@ashruti-msft"}, {"author": "jainakanksha-msft", "comment": "also rename the command file to genConfig to align with product design."}, {"author": "ashruti-msft", "comment": "@jainakanksha-msft made the reqd changes according to your suggestions lmk if this looks good."}]}
{"id": "pr-1470", "type": "pr", "title": "Added statfs for block-cache", "body": "Added statfs for block_cache which will show the memory available if disk is not configured otherwise the disk space. Added corresponding tests for this and fixed a few unit changes in file_cache.\r\n\r\nUsed a VM with an attached disk of size 5G for the follwoing outputs:\r\n\r\nWhen file cache/block cache uses the attached disk for disk caching\r\n![Screenshot 2024-11-22 153142](https://github.com/user-attachments/assets/ea315eb4-acfe-4478-8561-4facf3539576)\r\n\r\nWhen config has block_cache component without disk cache it will show the memory available:\r\n![Screenshot 2024-11-22 153029](https://github.com/user-attachments/assets/fb4fa16b-e436-48b8-8a97-e38e25be455e)\r\n\r\nOutput when file cache/block cache uses the same VM as disk cache\r\n![Screenshot 2024-11-22 152615](https://github.com/user-attachments/assets/6e443413-6b65-43e9-9cae-9c08e22e1e02)", "author": "ashruti-msft", "created_at": "2024-07-24T21:35:19+00:00", "comments": [{"author": "jainakanksha-msft", "comment": "@ashruti-msft Please add the output in the PR description and details of the change."}, {"author": "jainakanksha-msft", "comment": "@vibhansa-msft \r\nThis PR changes will represent temp disk details with the notation of root Filesystem.\r\nI think that can confuse the users.\r\nShould we change the FileSystem representation also for this somehow if that is feasible?"}, {"author": "ashruti-msft", "comment": "> @ashruti-msft Please add the output in the PR description and details of the change.\r\n\r\n@jainakanksha-msft I have added the output please have a look"}]}
{"id": "pr-1467", "type": "pr", "title": "Reset block when released", "body": "Reset the block data to null when it is released back to the block pool. As a result of this, a block retrieved from the block pool will have null data instead of garbage data.", "author": "souravgupta-msft", "created_at": "2024-07-22T18:25:20+00:00", "comments": [{"author": "souravgupta-msft", "comment": "> I think it would be better to first merge this pr [Block-cache read block correction\u00a0#1462](https://github.com/Azure/azure-storage-fuse/pull/1462) before merging this pr, as reading the block is causing us the issue, if it is fixed we can verify all other scenarios here as well.\r\n\r\nThis fix is for write and not related to read scenario. So, it is independent of the above PR.\r\n\r\n> Queries:\r\n>\r\n> 1. I believe, blobfuse is not POSIX complaint, It doesn't implement lseek(atleast for block cache). If we open a file, we write it sequentially and data integrity is preserved.\r\n>    but, if we seek to some offset and write at random offsets in the file handle we doesn't preserve data Integrity. It seems to me that the current fix is regarding this scenario.\r\n>    Ex: In blobfuse if my block size is 32MB, and I open a file and write some data at start say we created first block and now when we seek to 100MB offset and write at that position, I believe we corrupt the data even if we patch it with the current fix.\r\n\r\nWe have decided to fix the issues with random write in different PRs, where each PR will be fixing one sub-issue. This makes the code review easy. This PR just fixes the part where say first few bytes in a block are filled and the rest data is null. So, during releasing a block we clear the data in block to prevent this. The write issue in the example you mentioned is related to sparse files which will be fixed in the future PRs.\r\n\r\n\r\n> 2. I suppose that the current fix only works when user seeks in the allocated blocks, **I think it is better to mention that we don't support seek at all, hence user is aware of that.**\r\n\r\nWe will support seek in block cache. The fix for it will be coming in future PRs.\r\n\r\n> 3. are there any sequential write problems that we are fixing using this pr?\r\n\r\nMentioned in the answer for the first query."}]}
{"id": "pr-1453", "type": "pr", "title": "Cleanup stale mount in remount case", "body": "Auto cleanup stale mounts when remounting.", "author": "vibhansa-msft", "created_at": "2024-07-10T08:38:09+00:00", "comments": [{"author": "vibhansa-msft", "comment": "/azp run"}, {"author": "azure-pipelines[bot]", "comment": "<samp>\nAzure Pipelines successfully started running 1 pipeline(s).<br>\r\n\n</samp>"}, {"author": "vibhansa-msft", "comment": "/azp run"}, {"author": "azure-pipelines[bot]", "comment": "<samp>\nAzure Pipelines successfully started running 1 pipeline(s).<br>\r\n\n</samp>"}, {"author": "vibhansa-msft", "comment": "/azp run"}, {"author": "azure-pipelines[bot]", "comment": "<samp>\nAzure Pipelines successfully started running 1 pipeline(s).<br>\r\n\n</samp>"}, {"author": "vibhansa-msft", "comment": "/azp run"}, {"author": "azure-pipelines[bot]", "comment": "<samp>\nNo pipelines are associated with this pull request.<br>\r\n\n</samp>"}]}
{"id": "pr-1452", "type": "pr", "title": "ObjectID info updated and simplified base config", "body": "I have removed most options which i thought users may not use commonly. Let me know if I need to include/delete any config parameters. Can we remove any options under azstorage? it looks a bit cluttered right now.", "author": "ashruti-msft", "created_at": "2024-07-10T07:29:19+00:00", "comments": [{"author": "vibhansa-msft", "comment": "/azp run"}, {"author": "azure-pipelines[bot]", "comment": "<samp>\nAzure Pipelines successfully started running 1 pipeline(s).<br>\r\n\n</samp>"}]}
{"id": "pr-1416", "type": "pr", "title": "Autoconfig", "body": "Added calcultions for parameters for prefetch, parallelism, mem-size-mb, disk-size-mb in block_cache. \r\n Updated test cases according to new calculations.\r\n \r\n P.S: While printing blockCache configuations some values were in bytes and some in MB so I made the output uniform for all in bytes.", "author": "ashruti-msft", "created_at": "2024-05-16T12:07:02+00:00", "comments": [{"author": "souravgupta-msft", "comment": "Can you also write tests for validating these options that are auto computed. For example, you can compute free memory using the `free` command in Linux and compare the output against the one computed by the code."}]}
{"id": "pr-1384", "type": "pr", "title": "azcli auth mode", "body": "Adding authentication using Azure CLI", "author": "souravgupta-msft", "created_at": "2024-04-09T08:19:14+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Can we try running this mode on a system where cli is not installed and then see what it reports back?\r\nAlso, on a system where cli is not installed and mode is not set to cli it shall not fail due to any dependency or anything."}, {"author": "souravgupta-msft", "comment": "> Can we try running this mode on a system where cli is not installed and then see what it reports back? Also, on a system where cli is not installed and mode is not set to cli it shall not fail due to any dependency or anything.\r\n\r\nIf az cli is not installed, it will return back the error saying that \"az is not found\". I will add negative tests for this mode.\r\n\r\nWhat do you mean by this,\r\n\"Also, on a system where cli is not installed and mode is not set to cli it shall not fail due to any dependency or anything.\""}]}
{"id": "pr-1328", "type": "pr", "title": "Make logging of credentials in azauthmsi.go and azauthspn.go consistent", "body": "Currently the code in `component/azstorage/azauthmsi.go` logs sensitive data such as access tokens in the `log.Info` level, which can lead to sensitive data being present in the logs.\r\n\r\nAs having the access tokens can be useful for debugging, I'd like to propose this format, where the information that the token was successfully retrieved is logged in the `Info` level, but the token itself only in the `Debug` level.", "author": "leonardotbo", "created_at": "2024-01-26T09:52:26+00:00", "comments": [{"author": "leonardotbo", "comment": "@microsoft-github-policy-service agree"}]}
{"id": "pr-1310", "type": "pr", "title": "Dump all config in the logs", "body": "_In the most recent comment you can see the log_info generated when mounting by only file_cache_\r\n\r\n**Config dump in logs from each component: libfuse, attrcache, stream, filecache, blockcache, azstorage :**\r\n\r\nblobfuse2[1546781] : LOG_INFO [libfuse.go (244)]: Libfuse::Validate : UID 1000, GID 1000\r\nblobfuse2[1546781] : LOG_INFO [libfuse.go (305)]: Libfuse::Configure : read-only true, allow-other true, allow-root false, default-perm 511, entry-timeout 120, attr-time 120, negative-timeout 240, ignore-open-flags true, nonempty false, direct_io false, max-fuse-threads 128, fuse-trace false, extension , disable-writeback-cache false, dirPermission 511, mountPath /home/anubhuti/mntdir, umask 0\r\nblobfuse2[1546781] : LOG_INFO [stream.go (132)]: Stream::Configure : Buffer size 8, Block size 4, Handle limit 80, FileCaching false, Read-only true, StreamCacheMb 0, MaxBlocksPerFile 0\r\nblobfuse2[1546781] : LOG_INFO [file_cache.go (289)]: FileCache::Configure : Using default eviction policy\r\nblobfuse2[1546781] : LOG_INFO [file_cache.go (316)]: FileCache::Configure : create-empty false, cache-timeout 18000, tmp-path /home/anubhuti/tempcache, max-size-mb 1, high-mark 80, low-mark 60, refresh-sec 0, max-eviction 5000, hard-limit false, policy , allow-non-empty-temp false, cleanup-on-start false, policy-trace false, offload-io false, sync-to-flush false, ignore-sync true, defaultPermission -rwxrwxrwx, diskHighWaterMark 0, maxCacheSize 1, mountPath /home/anubhuti/mntdir\r\nblobfuse2[1546781] : LOG_INFO [block_cache.go (264)]: BlockCache::Configure : block size 33554432, mem size 4395630592, worker 128, prefetch 100, disk path , max size 4192MB, disk timeout 120, prefetch-on-open false, maxDiskUsageHit false, noPrefetch false\r\nblobfuse2[1546781] : LOG_INFO [attr_cache.go (156)]: AttrCache::Configure : cache-timeout 3600, symlink false, cache-on-list true, max-files 5000000\r\nblobfuse2[1546781] : LOG_INFO [config.go (407)]: ParseAndValidateConfig : using the following proxy address from the config file: \r\nblobfuse2[1546781] : LOG_INFO [config.go (411)]: ParseAndValidateConfig : sdk logging from the config file: false\r\nblobfuse2[1546781] : LOG_INFO [config.go (504)]: ParseAndValidateConfig : Account: anushrutiteststorage, Container: test-cnt, AccountType: BLOCK, Auth: SAS, Prefix: , Endpoint: https://anushrutiteststorage.blob.core.windows.net/, ListBlock: 0, MD5 : false false, Virtual Directory: true, Max Results For List 2, Disable Compression: false, CPK Enabled: false\r\nblobfuse2[1546781] : LOG_INFO [config.go (507)]: ParseAndValidateConfig : UseHTTP: false, BlockSize: 0, MaxConcurrency: 32, DefaultTier: , FailUnsupportedOp: true, mountAllContainers: false\r\nblobfuse2[1546781] : LOG_INFO [config.go (508)]: ParseAndValidateConfig : Retry Config: Retry count 5, Max Timeout 900, BackOff Time 4, Max Delay 60\r\nblobfuse2[1546781] : LOG_INFO [config.go (511)]: ParseAndValidateConfig : Telemetry : , Honour ACL: false, disable symlink: true", "author": "ashruti-msft", "created_at": "2024-01-10T11:55:09+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Verify once that all config is printed out properly. May be you can dump the logs emitted out, here in a comment so that we have a reference on what all is printed."}, {"author": "ashruti-msft", "comment": "**LOG_INFO found in logs when mounted by FILE_CACHE:**\r\n\r\n\r\n\r\n blobfuse2[1545364] : LOG_INFO [libfuse.go (244)]: Libfuse::Validate : UID 1000, GID 1000\r\n blobfuse2[1545364] : LOG_INFO [libfuse.go (305)]: Libfuse::Configure : read-only false, allow-other true, allow-root false, default-perm 511, entry-timeout 120, attr-time 120, negative-timeout 240, ignore-open-flags true, nonempty false, direct_io false, max-fuse-threads 128, fuse-trace false, extension , disable-writeback-cache false, dirPermission 511, mountPath /home/anubhuti/mntdir, umask 0\r\n blobfuse2[1545364] : LOG_INFO [file_cache.go (289)]: FileCache::Configure : Using default eviction policy\r\n blobfuse2[1545364] : LOG_INFO [file_cache.go (316)]: FileCache::Configure : create-empty false, cache-timeout 18000, tmp-path /home/anubhuti/tempcache, max-size-mb 1, high-mark 80, low-mark 60, refresh-sec 0, max-eviction 5000, hard-limit false, policy , allow-non-empty-temp false, cleanup-on-start false, policy-trace false, offload-io false, sync-to-flush false, ignore-sync true, defaultPermission -rwxrwxrwx, diskHighWaterMark 0, maxCacheSize 1, mountPath /home/anubhuti/mntdir\r\n blobfuse2[1545364] : LOG_INFO [attr_cache.go (156)]: AttrCache::Configure : cache-timeout 3600, symlink false, cache-on-list true, max-files 5000000\r\n blobfuse2[1545364] : LOG_INFO [config.go (407)]: ParseAndValidateConfig : using the following proxy address from the config file: \r\n blobfuse2[1545364] : LOG_INFO [config.go (411)]: ParseAndValidateConfig : sdk logging from the config file: false\r\n blobfuse2[1545364] : LOG_INFO [config.go (504)]: ParseAndValidateConfig : Account: anushrutiteststorage, Container: test-cnt, AccountType: BLOCK, Auth: SAS, Prefix: , Endpoint: https://anushrutiteststorage.blob.core.windows.net/, ListBlock: 0, MD5 : false false, Virtual Directory: true, Max Results For List 2, Disable Compression: false, CPK Enabled: false\r\n blobfuse2[1545364] : LOG_INFO [config.go (507)]: ParseAndValidateConfig : UseHTTP: false, BlockSize: 0, MaxConcurrency: 32, DefaultTier: , FailUnsupportedOp: true, mountAllContainers: false\r\n blobfuse2[1545364] : LOG_INFO [config.go (508)]: ParseAndValidateConfig : Retry Config: Retry count 5, Max Timeout 900, BackOff Time 4, Max Delay 60\r\n blobfuse2[1545364] : LOG_INFO [config.go (511)]: ParseAndValidateConfig : Telemetry : , Honour ACL: false, disable symlink: true\r\n blobfuse2[1545364] : LOG_INFO [mount.go (413)]: mount: Mounting blobfuse2 on /home/anubhuti/mntdir\r\n blobfuse2[1545400] : LOG_INFO [libfuse.go (244)]: Libfuse::Validate : UID 1000, GID 1000\r\n blobfuse2[1545400] : LOG_INFO [libfuse.go (305)]: Libfuse::Configure : read-only false, allow-other true, allow-root false, default-perm 511, entry-timeout 120, attr-time 120, negative-timeout 240, ignore-open-flags true, nonempty false, direct_io false, max-fuse-threads 128, fuse-trace false, extension , disable-writeback-cache false, dirPermission 511, mountPath /home/anubhuti/mntdir, umask 0\r\n blobfuse2[1545400] : LOG_INFO [file_cache.go (289)]: FileCache::Configure : Using default eviction policy\r\n blobfuse2[1545400] : LOG_INFO [file_cache.go (316)]: FileCache::Configure : create-empty false, cache-timeout 18000, tmp-path /home/anubhuti/tempcache, max-size-mb 1, high-mark 80, low-mark 60, refresh-sec 0, max-eviction 5000, hard-limit false, policy , allow-non-empty-temp false, cleanup-on-start false, policy-trace false, offload-io false, sync-to-flush false, ignore-sync true, defaultPermission -rwxrwxrwx, diskHighWaterMark 0, maxCacheSize 1, mountPath /home/anubhuti/mntdir\r\n blobfuse2[1545400] : LOG_INFO [attr_cache.go (156)]: AttrCache::Configure : cache-timeout 3600, symlink false, cache-on-list true, max-files 5000000\r\n blobfuse2[1545400] : LOG_INFO [config.go (407)]: ParseAndValidateConfig : using the following proxy address from the config file: \r\n blobfuse2[1545400] : LOG_INFO [config.go (411)]: ParseAndValidateConfig : sdk logging from the config file: false\r\n blobfuse2[1545400] : LOG_INFO [config.go (504)]: ParseAndValidateConfig : Account: anushrutiteststorage, Container: test-cnt, AccountType: BLOCK, Auth: SAS, Prefix: , Endpoint: https://anushrutiteststorage.blob.core.windows.net/, ListBlock: 0, MD5 : false false, Virtual Directory: true, Max Results For List 2, Disable Compression: false, CPK Enabled: false\r\n blobfuse2[1545400] : LOG_INFO [config.go (507)]: ParseAndValidateConfig : UseHTTP: false, BlockSize: 0, MaxConcurrency: 32, DefaultTier: , FailUnsupportedOp: true, mountAllContainers: false\r\n blobfuse2[1545400] : LOG_INFO [config.go (508)]: ParseAndValidateConfig : Retry Config: Retry count 5, Max Timeout 900, BackOff Time 4, Max Delay 60\r\n blobfuse2[1545400] : LOG_INFO [config.go (511)]: ParseAndValidateConfig : Telemetry : , Honour ACL: false, disable symlink: true\r\n blobfuse2[1545400] : LOG_INFO [mount.go (413)]: mount: Mounting blobfuse2 on /home/anubhuti/mntdir\r\n blobfuse2[1545400] : LOG_INFO [mount.go (619)]: Mount::startDynamicProfiler : Staring profiler on [localhost:6060]\r\n blobfuse2[1545400] : LOG_INFO [lru_policy.go (139)]: lruPolicy::StartPolicy : Policy set with 18000 timeout\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (180)]: Libfuse::initFuse : Mounting with fuse3 library\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (266)]: Libfuse::NotifyMountToParent : Notifying parent for successful mount\r\n blobfuse2[1545364] : LOG_INFO [mount.go (465)]: mount: Child [1545400] mounted successfully at /home/anubhuti/mntdir\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (273)]: Libfuse::libfuse_init : Kernel Caps : 52428763\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (280)]: Libfuse::libfuse_init : Enable Capability : FUSE_CAP_PARALLEL_DIROPS\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (286)]: Libfuse::libfuse_init : Enable Capability : FUSE_CAP_AUTO_INVAL_DATA\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (293)]: Libfuse::libfuse_init : Enable Capability : FUSE_CAP_READDIRPLUS\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (299)]: Libfuse::libfuse_init : Enable Capability : FUSE_CAP_ASYNC_READ\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (305)]: Libfuse::libfuse_init : Enable Capability : FUSE_CAP_SPLICE_WRITE\r\n blobfuse2[1545400] : LOG_INFO [libfuse_handler.go (317)]: Libfuse::libfuse_init : Enable Capability : FUSE_CAP_WRITEBACK_CACHE\r\n blobfuse2[1545400] : LOG_INFO [azstorage.go (290)]: AzStorage::StreamDir : Unblocked List API"}]}
{"id": "pr-1294", "type": "pr", "title": "Enhancement: Adding CPK support", "body": "- Allows support of mounting containers with CPK. \r\n- Usage : --cpk-enabled=true flag with CPK_ENCRYPTION_KEY and CPK_ENCRYPTION_KEY_SHA256 set in environment variable or config file.  \r\nNote: This change does not affect HNS accounts.", "author": "abhiguptacse", "created_at": "2023-11-28T10:15:40+00:00", "comments": [{"author": "ashruti-msft", "comment": "Hey, \r\nWhile running the CI pipeline a fail occurred, this is the backtrace for it:\r\n\r\n```\r\nRUN   TestBlockBlob/TestUploadBlobWithCPKEnabled\r\n    suite.go:87: test panicked: interface conversion: azstorage.AzConnection is nil, not *azstorage.BlockBlob\r\n        goroutine 6043 [running]:\r\n        runtime/debug.Stack()\r\n        \t/usr/local/go/src/runtime/debug/stack.go:24 +0x65\r\n        github.com/stretchr/testify/suite.failOnPanic(0xc004535ba0, {0xdb5100, 0xc0007011d0})\r\n        \t/home/vsts/go/pkg/mod/github.com/stretchr/testify@v1.8.4/suite/suite.go:87 +0x3b\r\n        github.com/stretchr/testify/suite.Run.func1.1()\r\n        \t/home/vsts/go/pkg/mod/github.com/stretchr/testify@v1.8.4/suite/suite.go:183 +0x252\r\n        panic({0xdb5100, 0xc0007011d0})\r\n        \t/usr/local/go/src/runtime/panic.go:884 +0x213\r\n        github.com/Azure/azure-storage-fuse/v2/component/azstorage.(*blockBlobTestSuite).setupTestHelper(0xc00037e700, {0xc0001306e0?, 0xc000187470?}, {0xc0009899f0?, 0x6?}, 0x0)\r\n        \t/home/vsts/work/1/s/component/azstorage/block_blob_test.go:233 +0x565\r\n        github.com/Azure/azure-storage-fuse/v2/component/azstorage.(*blockBlobTestSuite).TestUploadBlobWithCPKEnabled(0xc00037e700)\r\n\r\n```\r\n\r\nPlease look into this."}]}
{"id": "pr-1276", "type": "pr", "title": "Respect uid/gid sent over cli param or config file", "body": "- uid/gid supplied in config file are respected\r\n- uid/gid supplied over cli params are getting overwritten by the current user and group info during config validation phase.", "author": "vibhansa-msft", "created_at": "2023-10-27T04:41:58+00:00", "comments": [{"author": "vibhansa-msft", "comment": "UT addition for this component is not possbile so here is my UT done locally:\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$ ./blobfuse2 mount /usr/blob_mnt -o allow_other && ls -l /usr/blob_mnt && sudo ./blobfuse2 unmount all\r\ntotal 0\r\n-rw-r--r-- 1 vikas vikas 5444206592 Oct 25 14:51 tempfile_local.test\r\nSuccessfully unmounted /usr/blob_mnt\r\n1 of 1 mounts were successfully unmounted\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$ sudo ./blobfuse2 mount /usr/blob_mnt -o allow_other && ls -l /usr/blob_mnt && sudo ./blobfuse2 unmount all\r\ntotal 0\r\n-rw-r--r-- 1 root root 5444206592 Oct 25 14:51 tempfile_local.test\r\nSuccessfully unmounted /usr/blob_mnt\r\n1 of 1 mounts were successfully unmounted\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$ ./blobfuse2 mount /usr/blob_mnt -o allow_other,uid=1002,gid=1002 && ls -l /usr/blob_mnt && sudo ./blobfuse2 unmount all\r\ntotal 0\r\n-rw-r--r-- 1 faltoo faltoo 5444206592 Oct 25 14:51 tempfile_local.test\r\nSuccessfully unmounted /usr/blob_mnt\r\n1 of 1 mounts were successfully unmounted\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$ ./blobfuse2 mount /usr/blob_mnt -o allow_other,uid=0,gid=0 && ls -l /usr/blob_mnt && sudo ./blobfuse2 unmount all\r\ntotal 0\r\n-rw-r--r-- 1 root root 5444206592 Oct 25 14:51 tempfile_local.test\r\nSuccessfully unmounted /usr/blob_mnt\r\n1 of 1 mounts were successfully unmounted\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$ sudo ./blobfuse2 mount /usr/blob_mnt -o allow_other,uid=1002,gid=1002 && ls -l /usr/blob_mnt && sudo ./blobfuse2 unmount all\r\ntotal 0\r\n-rw-r--r-- 1 faltoo faltoo 5444206592 Oct 25 14:51 tempfile_local.test\r\nSuccessfully unmounted /usr/blob_mnt\r\n1 of 1 mounts were successfully unmounted\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$ sudo ./blobfuse2 mount /usr/blob_mnt -o allow_other,uid=1001,gid=1001 && ls -l /usr/blob_mnt && sudo ./blobfuse2 unmount all\r\ntotal 0\r\n-rw-r--r-- 1 nonsudo nonsudo 5444206592 Oct 25 14:51 tempfile_local.test\r\nSuccessfully unmounted /usr/blob_mnt\r\n1 of 1 mounts were successfully unmounted\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$ sudo ./blobfuse2 mount /usr/blob_mnt -o allow_other,uid=0,gid=0 && ls -l /usr/blob_mnt && sudo ./blobfuse2 unmount all\r\ntotal 0\r\n-rw-r--r-- 1 root root 5444206592 Oct 25 14:51 tempfile_local.test\r\nSuccessfully unmounted /usr/blob_mnt\r\n1 of 1 mounts were successfully unmounted\r\n\r\nvikas@vibhansa-ubn-vm:~/go/src/azure-storage-fuse$"}]}
{"id": "pr-1261", "type": "pr", "title": "Added content-type for .usdz file", "body": "Fixes #1260 \r\n\r\nFor the .glb file uploading from AzCopy also gives content-type as \"application/octet-stream\" so changes have been made only for .usdz file", "author": "ashruti-msft", "created_at": "2023-10-16T07:36:32+00:00", "comments": [{"author": "souravgupta-msft", "comment": "Should we have a changelog for this?\r\n@vibhansa-msft, @ashruti-msft"}, {"author": "vibhansa-msft", "comment": "> Should we have a changelog for this? @vibhansa-msft, @ashruti-msft\r\n\r\nNo need as this does not impact any functionality or not a new feature either."}]}
{"id": "pr-1180", "type": "pr", "title": "Fix function names in logs and log more", "body": "This PR fixes some of the logging in blobfuse2. Namely, that some functions did not provide trace logs, or they referred to the wrong function name when logging. I also tried to standardize some of the logging in files that were using different formats.", "author": "jfantinhardesty", "created_at": "2023-06-20T20:54:23+00:00", "comments": [{"author": "jfantinhardesty", "comment": "@microsoft-github-policy-service agree company=\"Seagate\""}, {"author": "vibhansa-msft", "comment": "Some of these logs were left intentionally as the regular workflow will hit these functions quite often and any sort of logging here will degrade the performance. getAttr/Read/Write/StreamDir are hit very heavily and any logging shall be avoided here.\r\nIs there any particular reason you wish to add these logs?"}, {"author": "jfantinhardesty", "comment": "It was mainly to standardize the logging. We were doing some debugging and reading through the log files and it was sometimes difficult to follow when we were expecting a function to log something, but it didn't. This lost us a bit of time trying to figure out why a certain function wasn't getting called, when we found that the trace for it didn't exist. Currently, the logging for these functions is a bit sporadic. For GetAttr every component except libfuse and stream do a trace log. For Read no component does a trace log. For Write only the Azstorage component does a trace log. And for StreamDir every component except file cache does a trace log.\r\n\r\nI think it would be helpful to make it clear which components will do a trace log for these functions and which ones don't. I agree that if there is a performance issue then not all of these should do a trace log then."}, {"author": "vibhansa-msft", "comment": "As I mentioned some of the things are done intentionally. For e.g. getAttr generally do not get pass through the attr-cache (as it has higher timeouts) so putting a log in azstorage allows us to navigate when what the getAttr actually called on storage. On the flows where we hit the most, we try to avoid loggging."}, {"author": "vibhansa-msft", "comment": "I see there are some wrong logs which you have tried to fix too. I suggest you remove logs that you have added from read/write/getattr/streamdir methods and remaining corrections can be taken up as part of this PR then."}, {"author": "jfantinhardesty", "comment": "Ok. I remove those added logs for read/write/getattr/streamdir."}, {"author": "vibhansa-msft", "comment": "Have provided some comments kindly fix those and it shall be good to go."}, {"author": "jfantinhardesty", "comment": "Thanks. I think I addressed all your comments."}]}
{"id": "pr-1149", "type": "pr", "title": "Performance Improvements for blobfuse attribute lookup", "body": "This PR adds new configs to blobfuse which impacts list and open operation time", "author": "shubham808", "created_at": "2023-05-22T12:08:10+00:00", "comments": [{"author": "souravgupta-msft", "comment": "Please add an entry in the changelog for this."}, {"author": "vibhansa-msft", "comment": "Can we also add some UT for validation of these values which we have made configurable to test their default values and when a config is set they are set with the configured values. There is a UT file for each component in the same directory and you can use that to define the UT."}, {"author": "vibhansa-msft", "comment": "There are go formatting errors in this PR and CI is not going through:\r\nhttps://dev.azure.com/azstorage/BlobFuse/_build/results?buildId=16775&view=logs&j=1fbcc5aa-e08c-514c-179d-87ec73246166&t=122e66bd-a102-5d27-e872-5aaed2361566"}]}
{"id": "pr-1088", "type": "pr", "title": "Fix: async (background) mount may cause problems", "body": "# Fix\r\n#1079 \r\nPod deletion will cause global mount point to disappear. The root cause is csi driver will bind mount the global mount point into Pod volume immediately after `blobfuse2 mount` return successfully. However, fuse is still mounting at background, so actually bind mount and fuse mount happened at the same time, this will lead to a problem that unmount the bind mount (when deleting a Pod) will cause the original mount point unmounted as well.\r\n#1081 \r\nMount is actually failed, but the errno returned by blobfuse2 is 0 and no error log (both terminal and log file).\r\n\r\n# Why\r\n1. libfuse provide `-f` option to decide whether running the process in foreground or as a daemon process. [Daemonizaion is based on `fork`](https://github.com/libfuse/libfuse/blob/master/lib/helper.c#L253-L305). \r\n2. blobfuse v2 uses cgo to compile blobfuse v2 and libfuse. However, [golang cannot handle damonization well](https://github.com/golang/go/issues/227), so v2 uses [go-daemon](https://github.com/sevlyar/go-daemon) as a workaround and always [pass `-f` option to avoid damonizing in libfuse](https://github.com/Azure/azure-storage-fuse/blob/main/component/libfuse/libfuse2_handler.go#L211-L217).\r\n3. [The daemonization in libfuse actually happens **AFTER** mount completed](https://github.com/libfuse/libfuse/blob/master/lib/helper.c#L351-L359), so the errorno and error message during mount stage can be returned successfully. Blobfuse v1 is written in C++, which can leverage the daemonization in libfuse, so blobfuse v1 can work normally.\r\n4. The difference in blobfuse v2 is that it uses go-daemon to daemonize **BEFORE** [fuse_main](https://github.com/libfuse/libfuse/blob/master/lib/helper.c#L307) (the entry point of libfuse), which makes [the mount operation and all the other operations before libfuse daemonization](https://github.com/libfuse/libfuse/blob/master/lib/helper.c#L316-L354) in background now, thus makes the mount error (and the other operations errors) invisible from user/csi driver. \r\n\r\n# How\r\n1. The `libfuse_init` callback can be an indicator that the mount operation has been successfully completed. At this point child process can send a signal to parent process to notify it that mount has been completed successfully and it can exit normally.\r\n2. When parent process receives `sigchild` signal, it means the child process exited unexpectedly, and the parent process will exit with error code and error message sent by child. \r\n3. Libfuse(child) print error log to stderr, ~~so I redirect the child process stderr to a temp file~~. By reading this file before exiting, parent process can print error message in terminal. BTW, `go-daemon` uses `forkExec` to fork and exec a new child process, so it's impossible to use PIPE for communicating since after exec, they won't share any opened file descriptor. \r\n4. Updated: We can pass the opened fd as arguments when exec the new child process. so, it is still possible to communicate between child and parent process through PIPE. The problem is that go-daemon does not support to set child stderr as an opened PIPE fd very well, I've filed a [PR](https://github.com/sevlyar/go-daemon/pull/90/files) to the project.\r\n\r\n\r\n# Test\r\nI've done test manually and it can solve the problem in both #1079 and #1081.\r\n\r\n\r\n# csi driver\r\nTo be safe, blob csi driver has to use `IsLikelyNotMountPoint` or `MountedFast` after executing `blobfuse2 mount` to make sure the mount point is really mounted successfully.  Refer: https://github.com/kubernetes-sigs/blob-csi-driver/pull/852", "author": "cvvz", "created_at": "2023-03-19T01:34:59+00:00", "comments": [{"author": "cvvz", "comment": "Since this [PR ](https://github.com/sevlyar/go-daemon/pull/90) has been merged by the maintainer of go-daemon, I think we can use PIPE now. @vibhansa-msft"}, {"author": "cvvz", "comment": "LGTM"}, {"author": "vibhansa-msft", "comment": "These changes are impacting the UT and code-coverage a lot :("}]}
{"id": "pr-1087", "type": "pr", "title": "Exit with non-zero status code if allow_other is used but not enabled in fuse config", "body": "Fix for #1081", "author": "vibhansa-msft", "created_at": "2023-03-17T11:46:43+00:00", "comments": [{"author": "cvvz", "comment": "Please take a look at https://github.com/Azure/azure-storage-fuse/pull/1088. I think it should be the right way to solve this kind of problem, instead of handling the `-o allow_other` option alone."}, {"author": "vibhansa-msft", "comment": "> Please take a look at #1088. I think it should be the right way to solve this kind of problem, instead of handling the `-o allow_other` option alone.\r\n\r\nThat other PR will not solve this issue as post those changes blobfuse binary will exit with non zero status code but still does not give any hint on what went wrong."}, {"author": "cvvz", "comment": "Since this PR has been merged and I'm not the expert of blobfuse V2, so I'm not going to oppose to this pr. \r\n\r\nHowever, I have to say that with #1088, we can sure get the error code and error message, which is **directly returned from libfuse**.\r\n![image](https://user-images.githubusercontent.com/44308864/226540879-f9cf65e8-8541-4b51-9c22-a8191e4953a6.png)\r\n\r\nThe other concern about this PR is that: if we decide to upgrade libfuse someday and some behavior changed, or if there are some differences from fuse v2 and fuse v3, we may still encounter some problem and have to add more similar changes in blobfuse2 validation logic."}, {"author": "andyzhangx", "comment": "@vibhansa-msft pls hold on releasing any new blobfuse2 version, we are setting up a daily e2e test validation on blobfuse2 main branch, to make sure there is no regression after integration with blob csi driver."}, {"author": "cvvz", "comment": "@vibhansa-msft Can you test with https://github.com/Azure/azure-storage-fuse/pull/1088 again to see if it can solve this issue? I've tested and it can provide error message from libfuse. Please rethink about this PR. Considering the risk I just raised, I'd prefer we solve this problem by   https://github.com/Azure/azure-storage-fuse/pull/1088, rather than this PR. Thanks"}]}
{"id": "pr-1052", "type": "pr", "title": "Create work dir if it does not exists", "body": "Create work dir if it does not exists", "author": "vibhansa-msft", "created_at": "2023-02-07T10:25:31+00:00", "comments": [{"author": "tasherif-msft", "comment": "was this related to a github issue?"}, {"author": "vibhansa-msft", "comment": "> was this related to a github issue?\r\n\r\nNot a github issue, but recently we made a change related to $HOME directory and because of that change we observed some errors in Healthmon where this directory does not existsand mount fails. Looking at that we realised earlier logging.Init() used to create the directory if it does not exist, but due to recent changes it was removed so added it back to validate() method."}]}
{"id": "pr-1001", "type": "pr", "title": "Minor Fixes", "body": "- [#999](https://github.com/Azure/azure-storage-fuse/issues/999) Upgrade dependencies to resolve known CVEs.\r\n- [#1002](https://github.com/Azure/azure-storage-fuse/issues/1002) In case version check fails to connect to public container, dump a log to check network and proxy settings.\r\n- [#1006](https://github.com/Azure/azure-storage-fuse/issues/1006) Remove user and group config from logrotate file.", "author": "vibhansa-msft", "created_at": "2022-12-12T04:49:42+00:00", "comments": [{"author": "vibhansa-msft", "comment": "@andyzhangx : kindly review this and provide your feedback."}, {"author": "andyzhangx", "comment": "btw, pls also squash commits into a few commits, make commits history tidy"}]}
{"id": "pr-967", "type": "pr", "title": "[fixed] unexpected maximum might incur security issues.", "body": "Hi Blobfuse team,\r\nFound a case that might be missed: \r\nwhen calling `BlobStreamer::GetBlock` if `start_offset `> `obj->GetSize()`, `download_size `will get a maximum of `download_size`'s type.\r\n\r\nLike when `block_size` equals 512 * 1024 * 1024, `offset` equals 512 * 1024 * 1024 + 100, `obj->GetSize()` equals 10. \r\n`start_offset ` will get 512 * 1024 * 1024 - 100 and it gonna execute` download_size = obj->GetSize() - start_offset;` .`obj->GetSize() - start_offset` logically would get negative. \r\nBut, if we assign the negative value to `download_size `of type uint64_t, `download_size ` will be converted to UINT64_MAX. \r\n\r\nI think this should be an unexpected behavior. The risk of memory leak might incur security issues.\r\n\r\nI hope it helps thx,\r\njack", "author": "heiaun-jack", "created_at": "2022-11-04T07:24:17+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Are you hitting this case or just found this as a safety check?"}, {"author": "heiaun-jack", "comment": "> Are you hitting this case or just found this as a safety check?\r\n\r\nWant to tune the performance via the option (storage/ cache/ stream) and want to know better how the options work, then start reading the code. \r\n\r\nI came across the issue and think it is necessary to point it out."}, {"author": "vibhansa-msft", "comment": "> > Are you hitting this case or just found this as a safety check?\r\n> \r\n> Want to tune the performance via the option (storage/ cache/ stream) and want to know better how the options work, then start reading the code.\r\n> \r\n> I came across the issue and think it is necessary to point it out.\r\n\r\nNice catch. We can put the fix once review comments have been addressed. However, one thing to note here is that we are working on next generation of blobfuse aka blobfuse2 and we are not planning to publish any new release for our legacy code, unless there is a high severity bug. Feature requests we stopped taking around a year back, so slowly we want all our customers to move to blobfuse2."}, {"author": "heiaun-jack", "comment": "> > > Are you hitting this case or just found this as a safety check?\r\n> > \r\n> > \r\n> > Want to tune the performance via the option (storage/ cache/ stream) and want to know better how the options work, then start reading the code.\r\n> > I came across the issue and think it is necessary to point it out.\r\n> \r\n> Nice catch. We can put the fix once review comments have been addressed. However, one thing to note here is that we are working on next generation of blobfuse aka blobfuse2 and we are not planning to publish any new release for our legacy code, unless there is a high severity bug. Feature requests we stopped taking around a year back, so slowly we want all our customers to move to blobfuse2.\r\n\r\ndone, thanks for sharing this"}]}
{"id": "pr-965", "type": "pr", "title": "Add retry in rename call when deletion fails and correct retry policy", "body": "Potential fix for #964", "author": "vibhansa-msft", "created_at": "2022-11-04T05:52:01+00:00", "comments": [{"author": "gapra-msft", "comment": "Please add the change in retry defaults to the baseConfig and the Changelog as a \"breaking change\""}]}
{"id": "pr-957", "type": "pr", "title": "Fix line to disable medium file for stream direct", "body": "Test case takes too long, so we will rely on small files for data validation for stream direct case.", "author": "gapra-msft", "created_at": "2022-11-02T16:07:54+00:00", "comments": [{"author": "gapra-msft", "comment": "Successful data validation run https://dev.azure.com/azstorage/BlobFuse/_build/results?buildId=14760&view=results"}]}
{"id": "pr-898", "type": "pr", "title": "Fixed parsing of endpoint to support blob endpoints for ADLS accounts", "body": "Resolves https://github.com/Azure/azure-storage-fuse/issues/894", "author": "gapra-msft", "created_at": "2022-09-08T18:52:32+00:00", "comments": [{"author": "tasherif-msft", "comment": "looks good"}]}
{"id": "pr-872", "type": "pr", "title": "Added scripts to generate output conveniently", "body": "Covers git clone and upload download cases so far. Next PR to include fio", "author": "gapra-msft", "created_at": "2022-08-18T23:08:13+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Our ResNet50 pipeline shall be taken as a perftest pipeline and may be these kinds of tests we can add there so that on weekly basis from our main branch this perftest and comparison against v1 is done. Periodically or on any major change we can just take the results from here and see if anything has gone up or down."}]}
{"id": "pr-837", "type": "pr", "title": "Dont delete directory in case of gen2 account if directory is not empty", "body": "Commands like \"git stash\" try attempting a non-empty directory. On regular filesystem it fails with error \"directory on empty\"\r\nIn case of blobfuse, for block blob account we check for non-empty directory condition but for gen2 account we go ahead and delete the directory recursively on container as well, this results into \"git stash\" command deleting \".git\" directory and resulting into a corrupting git repository.", "author": "vibhansa-msft", "created_at": "2022-07-14T15:25:29+00:00", "comments": [{"author": "gapra-msft", "comment": "This shouldn't be an issue in v2 right? We have this check before DeleteDir https://github.com/Azure/azure-storage-fuse/blob/main/component/libfuse/libfuse_handler.go#L505"}]}
{"id": "pr-832", "type": "pr", "title": "Code coverage threshold check", "body": "- Fixed code coverage pipeline for Ubuntu-20\r\n- Check overall code coverage\r\n- Check file level code coverage", "author": "souravgupta-msft", "created_at": "2022-07-07T10:25:44+00:00", "comments": [{"author": "gapra-msft", "comment": "@souravgupta-msft might be good to also note down any testing deficiencies we need to cover and create items for them"}, {"author": "vibhansa-msft", "comment": "As a result of this pipeline last step lists all the files that have less then 80% coverage. So, in a way we are creating a list where scope of testing is still there. Maybe we can just list them all in a work item and keep removing them as it increases."}]}
{"id": "pr-818", "type": "pr", "title": "[BUG] ParseFloat fix for file cache on locales that provide a \",\"", "body": "This resolves https://github.com/Azure/azure-storage-fuse/issues/784\r\nsome locales produce sizes of format `X,XM`\r\nReplace `,` to `.`", "author": "tasherif-msft", "created_at": "2022-06-17T21:10:14+00:00", "comments": [{"author": "gapra-msft", "comment": "Good job finding this fix!"}]}
{"id": "pr-809", "type": "pr", "title": "Added doc generation command", "body": "This command will be used internally to help support our docs team.", "author": "tasherif-msft", "created_at": "2022-06-12T06:47:15+00:00", "comments": [{"author": "gapra-msft", "comment": "Can you show us an example of what this looks like?"}, {"author": "tasherif-msft", "comment": "> Can you show us an example of what this looks like?\r\n\r\nYou can run it locally to check it out what it outputs. This will be used primarily by the docs team."}, {"author": "souravgupta-msft", "comment": "Should we add a test case for this command?"}, {"author": "gapra-msft", "comment": "> > Can you show us an example of what this looks like?\r\n> \r\n> You can run it locally to check it out what it outputs. This will be used primarily by the docs team.\r\n\r\nJust tried it out, it looks good. There is some improvement we could make on the source code end eventually that we can create an item for."}]}
{"id": "pr-807", "type": "pr", "title": "Streaming small fixes and minor perf improvement", "body": "- delay flush on appended data\r\n- fixed nil pointer issue in pure streaming\r\n- lock on stage+commit to avoid storage throwing on race\r\n- added more tests", "author": "tasherif-msft", "created_at": "2022-06-07T23:38:38+00:00", "comments": [{"author": "gapra-msft", "comment": "Can you add an item to the changelog"}]}
{"id": "pr-751", "type": "pr", "title": "Fixed bug where blobfuse2 could not be installed in docker container", "body": "The postinstall script was generating various problems so we now default to ship the postinstall script in /usr/share/blobfuse2/\r\nFixes : #733", "author": "gapra-msft", "created_at": "2022-03-28T19:01:17+00:00", "comments": [{"author": "gapra-msft", "comment": "Resolves #733"}, {"author": "andyzhangx", "comment": "could you also add a unit test mentioned here: https://github.com/Azure/azure-storage-fuse/issues/733#issuecomment-1068669562, thanks."}, {"author": "gapra-msft", "comment": "> could you also add a unit test mentioned here: [#733 (comment)](https://github.com/Azure/azure-storage-fuse/issues/733#issuecomment-1068669562), thanks.\r\n\r\nTest should be here\r\nhttps://github.com/Azure/azure-storage-fuse/pull/751/files#diff-e2bc9cbe33a01f52bdbc76c002cf2a5ad1268599a0ad303dc4b3812f390910f6R969"}]}
{"id": "pr-737", "type": "pr", "title": "File-cache open-file complexity with respect to file locks removed", "body": "- file-cache needed ex-locks on files before download or deletion\r\n- these ex-locks works only when file has write permission\r\n- to handle ex-lock and permission combination there was a lot of locks and complex logic involved in openFile. createFile and cache eviction logic\r\n- Moving to a solution where open handle counter is maintained inside the file level locks and using those we take the ex locks.", "author": "vibhansa-msft", "created_at": "2022-03-14T09:25:06+00:00", "comments": [{"author": "tasherif-msft", "comment": "so now we don't need the fs ex locks and we just use the high-level file name lock - cleaner approach!\r\nfor the handle tracking - this is to just ensure we delete from cache when handle counter is 0"}]}
{"id": "pr-732", "type": "pr", "title": "V2 Longhaul scripts", "body": "Adding scripts to run v2 longhaul\r\n- Modify these scripts as per local path\r\n- setup these in cronjob to run at regular internvals", "author": "vibhansa-msft", "created_at": "2022-03-12T07:51:57+00:00", "comments": [{"author": "tasherif-msft", "comment": "so this would just be run manually when we wanna do a longhaul run on a vm?"}]}
{"id": "pr-714", "type": "pr", "title": "Fixed bug where some user provided values that can be set to 0 were s\u2026", "body": "\u2026et to default value", "author": "gapra-msft", "created_at": "2022-02-22T23:15:13+00:00", "comments": [{"author": "tasherif-msft", "comment": "can we add some tests for this change? For example, explicitly pass 0 in config file and ensure default is actually passed (120 for example)"}]}
{"id": "pr-708", "type": "pr", "title": "Auto update of version on release", "body": "Adding a stage in the release pipeline which will update the version number in the latestVersion.json file", "author": "souravgupta-msft", "created_at": "2022-02-16T10:46:04+00:00", "comments": [{"author": "souravgupta-msft", "comment": "@gapra-msft , what convention will we be using for preview releases, **-preview** or **~preview**?\r\nAs of now I see that for both tag and version we are using **-preview**. So, going forward will we be going with the same convention?"}, {"author": "gapra-msft", "comment": "> @gapra-msft , what convention will we be using for preview releases, **-preview** or **~preview**? As of now I see that for both tag and version we are using **-preview**. So, going forward will we be going with the same convention?\r\n\r\nYeah we are going with -preview"}]}
{"id": "pr-704", "type": "pr", "title": "Perf and Memory Optimizations", "body": "Optimizations :\r\n- Instead of returning handle ID to libfuse return the handle object address itself so that on read/write/flush/close calls we can save from map lookup to convert handl id to handle object\r\n- Adding a new config which allows libfuse to directly read from locally cached file instead of forwarding request up in the pipeline\r\n- various structures were not aligned properly leaving lot of padding in between, rearranged fields to minimize the memory usage\r\n- lot of flags are used in handle/attr-cache/file-cache maps. Unified them all to use a bitmap instead of individual flags to conserve memory.\r\n- sleep on file cache tests where we expect the file to take time to be deleted", "author": "vibhansa-msft", "created_at": "2022-02-15T06:48:24+00:00", "comments": [{"author": "gapra-msft", "comment": "Looks like theres some problem with the libfuse3 tests. Please take a look at that and resolve it. Also could you rename the PR name to be a little more descriptive?"}, {"author": "tasherif-msft", "comment": "We should add a test to confirm that libfuse doesn't hit file cache when the flag is set"}]}
{"id": "pr-697", "type": "pr", "title": "Showing warnings for old versions", "body": "Show security warnings to the user if they are using older version of blobfuse2.", "author": "souravgupta-msft", "created_at": "2022-02-11T13:53:48+00:00", "comments": [{"author": "gapra-msft", "comment": "@souravgupta-msft can you merge main back into your branch? I messed up a little."}]}
{"id": "pr-650", "type": "pr", "title": "Fixed parsing of storage endpoint to support RA-GRS endpoints passed \u2026", "body": "\u2026through the blobEndpoint option", "author": "gapra-msft", "created_at": "2021-09-24T21:09:55+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Looks fine."}]}
{"id": "pr-641", "type": "pr", "title": "Vibhansa/read stream parallel download block", "body": "If the block size if bigger then 64MB, instead of downloading in single thread, download in parallel for better performance.", "author": "vibhansa-msft", "created_at": "2021-09-06T02:29:42+00:00", "comments": [{"author": "tasherif-msft", "comment": "I'm curious if we should have a separate max-concurrency attr specific to streaming. If the customer sets it then that means regardless of their block size they want to parallelize. This also enables us to avoid the hardcoded 64MB"}, {"author": "vibhansa-msft", "comment": "> I'm curious if we should have a separate max-concurrency attr specific to streaming. If the customer sets it then that means regardless of their block size they want to parallelize. This also enables us to avoid the hardcoded 64MB\r\n\r\nWe do not want user to do parallel download even for smaller buffer sizes. if we leave the option open, you can configure even smaller chunks to be downloaded by large number of threads, which will be overkill. So unless chunk size is >64MB we do not want to do things in parallel. This is matching with current blobfuse upload code where files upto 64MB are not broken down into blocks and are downloaded by single thread as single block. Once block size is bigger we try to do stuff in parallel. Reason for reusing the same config param is when streaming is enable that option will be of no use in regular flow, and as it control the same thing (number of parallel threads to run to download/upload file) just reused the same."}]}
{"id": "pr-572", "type": "pr", "title": "Vibhansa/block upload of unchanged file", "body": "Do not upload file unless it has been modified.\r\nUser can choose this behavior using \"--upload-if-modified=true\" cli option.", "author": "vibhansa-msft", "created_at": "2021-02-24T12:13:12+00:00", "comments": [{"author": "vibhansa-msft", "comment": "Only list api is blocked so if application tries to access any file immediately after load, it will still work. Objective was to only block the list call to stop implicit listing done by libfuse."}]}
{"id": "pr-473", "type": "pr", "title": "Update README.md with authType SPN", "body": "I am proposing this change as the content didn't had any information on authType 'SPN' that we can use for Service Principal Authentication. The connection.cfg has details where it is mentioned that SPN should be used as authType. However, since README.md had no information provided on it, I have added those changes.", "author": "sihegde", "created_at": "2020-09-18T07:51:22+00:00", "comments": [{"author": "sihegde", "comment": "Hi @vibhansa-msft ,\r\n\r\nThanks for the update! yes, both Config File options and Environment Variables sections have details about the Service principal authentication. However, it is bit confusing as there is no mention of 'authType' that has to be used for this. So I have added only 'authType' part."}, {"author": "vibhansa-msft", "comment": "> Hi @vibhansa-msft ,\r\n> \r\n> Thanks for the update! yes, both Config File options and Environment Variables sections have details about the Service principal authentication. However, it is bit confusing as there is no mention of 'authType' that has to be used for this. So I have added only 'authType' part.\r\n\r\nYes for the second part of the change I do agree with you."}, {"author": "vibhansa-msft", "comment": "I will approve this change-set and merge. We will re-phrase the first part of your change-set if required."}, {"author": "sihegde", "comment": "@vibhansa-msft ,\r\nThank you!  sure.."}]}
{"id": "pr-416", "type": "pr", "title": "Build/fix1804", "body": "not ready for review. Attempting to merge", "author": "NaraVen", "created_at": "2020-06-04T05:29:09+00:00", "comments": [{"author": "NaraVen", "comment": "I moved the sudo fix up and the build is working."}]}
{"id": "pr-411", "type": "pr", "title": "Bug/errormessage file check", "body": "Github #410 and Github #364 fixes\r\n\r\nThere are fixes for typos in error messages \r\nThe build for 19.04 is added to the Yml\r\nA bug fix identified while testing in getAttr is fixed too.", "author": "NaraVen", "created_at": "2020-05-27T05:51:49+00:00", "comments": [{"author": "vibhansa-msft", "comment": "In pipeline file some syntax issue is there, integration test has filed while running it."}]}
{"id": "pr-404", "type": "pr", "title": "blog to blob, fixes #403", "body": "fixes #403", "author": "tomecho", "created_at": "2020-05-18T17:10:55+00:00", "comments": [{"author": "msftclas", "comment": "[![CLA assistant check](https://cla.opensource.microsoft.com/pull/badge/signed)](https://cla.opensource.microsoft.com/Azure/azure-storage-fuse?pullRequest=404) <br/>All CLA requirements met."}]}
{"id": "pr-394", "type": "pr", "title": "Bug/msi client", "body": "This PR has the following changes\r\n1) fixed blobnotfound error related to auth after token expiry\r\n2) fixed blobnotfound error due to Getproperty call on dirs\r\n3) fixedparam unexplainable parsing issue for authtypes.\r\n4) removed unnecessary repeat call to getblobproperty and Get_Hierarchical_list", "author": "NaraVen", "created_at": "2020-05-04T09:50:29+00:00", "comments": [{"author": "NaraVen", "comment": "> If you leave the connection.cfg file as is (aka if you use the connection.cfg as it is shown in the repo) it will try to get the oauth token. It should be throwing an error right then and there and should not attempt to get an oauth token.\r\n> \r\n> There's a breaking change where if I don't specify the authType in the configuration file (just using accountName, accountKey and containerName), it will default to calling msi auth and prints out unnecessary statements showing it is trying to get the oauth token.\r\n> \r\n> We can take this testing offline, but these errors are not hard to reproduce.\r\n\r\nI tested with accountkey and secret pure commandline without config it worked.\r\nI modified the config with the right values including Key for authType and it worked, it did not default to msi. \r\nI don't understand your concern. The connection.cfg is just a placeholder it should never be used as is, that willl be obvious to anyone who opens it."}, {"author": "amnguye", "comment": "Try testing it with no secret. I was able to reproduce this with no environment variables and populating the configuration.cfg file with the accountName, accountKey and containerName.\r\n\r\nYes the connection.cfg should never be used as is, however it is a possibility the user will forget to populate their configuration file or save it. When that happens, it will try to retrieve the oauth token endlessly, and the user will be confused. Blobfuse should react by throwing an authentication error right away. This is just a base test case."}, {"author": "NaraVen", "comment": "Fixing the config is not part of this PR. I agree it is a well known bug, I will create a work item and fix it in another PR. This PR is for BlobNotFound issues. The two issues are unrelated.\n\nGet Outlook for iOS<https://aka.ms/o0ukef>\n________________________________\nFrom: Amanda Nguyen <notifications@github.com>\nSent: Tuesday, May 5, 2020 9:28:44 AM\nTo: Azure/azure-storage-fuse <azure-storage-fuse@noreply.github.com>\nCc: Nara V <narven@microsoft.com>; Author <author@noreply.github.com>\nSubject: Re: [Azure/azure-storage-fuse] Bug/msi client (#394)\n\n\nTry testing it with no secret. I was able to reproduce this with no environment variables and populating the configuration.cfg file with the accountName, accountKey and containerName.\n\nYes the connection.cfg should never be used as is, however it is a possibility the user will forget to populate their configuration file or save it. When that happens, it will try to retrieve the oauth token endlessly, and the user will be confused. Blobfuse should react by throwing an authentication error right away. This is just a base test case.\n\n\u2014\nYou are receiving this because you authored the thread.\nReply to this email directly, view it on GitHub<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2Fazure-storage-fuse%2Fpull%2F394%23issuecomment-624158986&data=02%7C01%7Cnarven%40microsoft.com%7Cc41a9e045db84677993008d7f111615c%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637242929260165232&sdata=aa3G8ZmlDcFKGQSWSV4jDRMxLReVwf%2Fbd3cANja7cNM%3D&reserved=0>, or unsubscribe<https://nam06.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAD5FI4HDPXHESCKDBMVG5QDRQA5DZANCNFSM4MYTPC5Q&data=02%7C01%7Cnarven%40microsoft.com%7Cc41a9e045db84677993008d7f111615c%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637242929260165232&sdata=yHqulnWKgPqH5JHuXyvv5TIOYnW4X7V8Vy%2Fz%2BRjwnnU%3D&reserved=0>."}, {"author": "amnguye", "comment": "These issues are related because they are introduced in this PR. When I pull master and build and run that, they are not produced."}, {"author": "NaraVen", "comment": "I cannot reproduce this Amanda. I used the same test criteria you used. Let us wait for Vikas Bhansali @vibhansa-msft  to come online and give this a try. \r\nWhen I specify the containerName, accountName and accountKey in the config file it mounts and runs correctly.\r\nWhen I specify the containerName, accountName only it gives me a failed to auth error and does not mount which is correct.\r\nI never run into the scenario where it keeps trying to retrieve an OAuthToken and confuse the user.\r\nSetup details -------------------------\r\nI do not have any environment variables for AZURE_STORAGE_ACCOUNT or SECRET \r\nThe config file looks as below.\r\naccountName fake9890\r\ncontainerName fake-nara-blobstore-3cd50821-00bf-483d-b11f-2ac7d3ae6841\r\naccountKey fakeKey----------"}, {"author": "NaraVen", "comment": "It worked in MSI, non-MSI with storage account and key on both 18.04 and 16.04 so I am finishing the PR"}]}
{"id": "pr-386", "type": "pr", "title": "Bug msiexpireson only auth", "body": "This PR has changes to the Authtoken expiry date handling\r\nexpires_in integer value is evaluated first. If it is available expires_on is not refered, it is calculated and set form expires_in\r\nThere is added feature to parse date formats in string for the expires_on field. \r\nThere are some helped methods to evaluate if a json value is indeed an integer\r\nThere is some module separation to create testability for the istoken_expired.\r\nThere are googletests added for OAuthToken.cpp and OAuthTokenCredential manager.\r\nThere are extra error logging messages even in CPPLite\r\nThe are changes to CMakeLists.sh to get googletests working\r\nThere are changes to the build.sh to ignore CPP warnings for C++11 , include the new tests and to compile tests..", "author": "NaraVen", "created_at": "2020-04-24T01:06:27+00:00", "comments": [{"author": "amnguye", "comment": "NIT - I think the tabbing might be on instead of spaces on your IDE, cause the formatting looks all over the place."}, {"author": "NaraVen", "comment": "> NIT - I think the tabbing might be on instead of spaces on your IDE, cause the formatting looks all over the place.\r\n\r\nconverted tabs to spaces. Should be easier on the eyes now."}, {"author": "amnguye", "comment": "Thanks for your changes Nara they look good! Most of my comments are nitpicky formatting stuff."}]}
{"id": "pr-350", "type": "pr", "title": "Systemd service file", "body": "Related to https://github.com/Azure/azure-storage-fuse/issues/347", "author": "scegg", "created_at": "2019-12-31T06:32:07+00:00", "comments": [{"author": "msftclas", "comment": "[![CLA assistant check](https://cla.opensource.microsoft.com/pull/badge/signed)](https://cla.opensource.microsoft.com/Azure/azure-storage-fuse?pullRequest=350) <br/>All CLA requirements met."}]}
{"id": "pr-325", "type": "pr", "title": "Adding Boost Filesystem normalization to tmp-path [#233]", "body": "Attempting to resolve #233 with argument checking. This is a modification of blobfuse.cpp to validate the tmp-path inputs by using boost::filesystem::path to normalize the path (remove excess slashes, double-dot, and dot). We also add in an extra check for empty paths after the modifications to ensure it didn't resolve to root (/) or an empty path.", "author": "nerddtvg", "created_at": "2019-11-04T18:26:13+00:00", "comments": [{"author": "msftclas", "comment": "[![CLA assistant check](https://cla.opensource.microsoft.com/pull/badge/signed)](https://cla.opensource.microsoft.com/Azure/azure-storage-fuse?pullRequest=325) <br/>All CLA requirements met."}]}
{"id": "pr-281", "type": "pr", "title": "feat: add AZURE_BLOB_ENDPOINT env support", "body": "<!--  Thanks for sending a pull request!  Here are some tips for you:\r\n\r\n1. If this is your first time, please read our contributor guidelines: https://git.k8s.io/community/contributors/guide#your-first-contribution and developer guide https://git.k8s.io/community/contributors/devel/development.md#development-guide\r\n2. Please label this pull request according to what type of issue you are addressing, especially if this is a release targeted pull request. For reference on required PR/issue labels, read here:\r\nhttps://git.k8s.io/community/contributors/devel/release.md#issue-kind-label\r\n3. Ensure you have added or ran the appropriate tests for your PR: https://git.k8s.io/community/contributors/devel/testing.md\r\n4. If you want *faster* PR reviews, read how: https://git.k8s.io/community/contributors/guide/pull-requests.md#best-practices-for-faster-reviews\r\n5. Follow the instructions for writing a release note: https://git.k8s.io/community/contributors/guide/release-notes.md\r\n6. If the PR is unfinished, see how to mark it: https://git.k8s.io/community/contributors/guide/pull-requests.md#marking-unfinished-pull-requests\r\n-->\r\n\r\n**What type of PR is this?**\r\n/kind feature\r\n\r\n**What this PR does / why we need it**:\r\nThis PR add `AZURE_BLOB_ENDPOINT` env to support different azure env\r\n\r\n**Which issue(s) this PR fixes**:\r\n<!-- \r\n*Automatically closes linked issue when PR is merged.\r\nUsage: `Fixes #<issue number>`, or `Fixes (paste link of issue)`.\r\n_If PR is about `failing-tests or flakes`, please post the related issues/tests in a comment and do not use `Fixes`_*\r\n-->\r\nFixes #\r\n\r\n**Special notes for your reviewer**:\r\n\r\n\r\n**Release note**:\r\n```\r\nfeat: add AZURE_BLOB_ENDPOINT env support\r\n```", "author": "andyzhangx", "created_at": "2019-06-19T06:56:25+00:00", "comments": [{"author": "andyzhangx", "comment": "@seguler I found there is even on build test on this repo? I am not sure how to build this repo..."}, {"author": "amnguye", "comment": "On the second part of the page, it shows you how to build the source\r\nhttps://github.com/Azure/azure-storage-fuse/wiki/1.-Installation\r\n\r\nAs for the tests, you can add to the python tests."}, {"author": "andyzhangx", "comment": "@amnguye thanks for the hint, I could compile now. \r\nWhy not put this build process into CI pipeline, so it would automatically build for every PR. I could help do that if you could add me as repo admin, it only needs a little work.\r\nWithout CI, new PR may break build easily."}, {"author": "andyzhangx", "comment": "I have verified this PR works well in Azure China Cloud by:\r\n```\r\nexport AZURE_BLOB_ENDPOINT=andyeast2.blob.core.chinacloudapi.cn\r\nexport AZURE_STORAGE_ACCOUNT=andyeast2\r\nexport AZURE_STORAGE_ACCESS_KEY=...\r\nmkdir blob\r\n./blobfuse blob --container-name=public --tmp-path=/tmp/blobfuse/\r\n```\r\n\r\nPls note that the AZURE_BLOB_ENDPOINT format should be `accountname.blob.core.chinacloudapi.cn` for Azure China Cloud"}, {"author": "amnguye", "comment": "I know you verified it with a working account and endpoint but I cannot approve this PR unless you write tests for this new feature. In the test you don't have to add in an actual account, key or endpoint, it can just be example text."}, {"author": "andyzhangx", "comment": "> I know you verified it with a working account and endpoint but I cannot approve this PR unless you write tests for this new feature. In the test you don't have to add in an actual account, key or endpoint, it can just be example text.\r\n\r\nHi @amnguye I am not sure how to add such a test into https://github.com/Azure/azure-storage-fuse/blob/master/blobfuse/tests.py, could you help? Thanks."}]}
{"id": "pr-259", "type": "pr", "title": "Fix by value exception catching", "body": "Fix compilation error with GCC 8.2.1 :\r\n\r\n> error: catching polymorphic type 'class std::exception' by value [-werror=catch-value=]\r\n\r\nCompiled successfully after modification", "author": "johnBuffer", "created_at": "2019-03-22T02:21:29+00:00", "comments": [{"author": "seguler", "comment": "@johnBuffer - could you accept the following contribution license agreement ? Once you do that, we'll merge this.\r\n\r\nhttps://cla.opensource.microsoft.com/Azure/azure-storage-fuse?pullRequest=259"}, {"author": "johnBuffer", "comment": "@seguler apparently I already did, when I follow your link everything is already filled"}]}
{"id": "pr-198", "type": "pr", "title": "Adding a sample AzureUSGovernment config file with blobEndpoint config example", "body": "I'll add some text in the wiki documentation to reference & explain this as well. Thank you for adding the Sovereign Cloud capability - I've had customers complain that the behavior for the blobEndpoint config is not well documented, so I'm attempting to flesh it out.", "author": "daweins", "created_at": "2018-07-11T19:35:58+00:00", "comments": [{"author": "seguler", "comment": "Thanks @daweins"}]}
{"id": "pr-183", "type": "pr", "title": "Building azure-storage-cpp-lite standalone", "body": "I was interested in using just the azure-storage-cpp-lite code but ran into a few build issues on my mac. Hopefully this is a good change for the project as well, but focuses around error fixes on build and reduced dependency requirement for that component\r\n\r\n- azure-storage-cpp-lite links against OpenSSL so use that to do HMAC hash and able to remove additional dependency on GnuTLS\r\n- Build also failed with a few issues just from overrides and unused lambda captures\r\n- Also building sample with cmake\r\n\r\nSteps to reproduce build fail:\r\n```bash\r\ncd azure-storage-cpp-lite\r\nmkdir build && cd build\r\ncmake ..\r\nmake azure-storage\r\n```", "author": "damienpontifex", "created_at": "2018-05-14T03:51:20+00:00", "comments": [{"author": "seguler", "comment": "Awesome! You're great! \r\n\r\nCouple of issues:\r\n- We intentionally moved to GnuTLS because its license plays better with the license we have. We cannot accept the change to OpenSSL\r\n- Why was there a need to upgrade to Cmake 3.5 ? Most users on older distros only have Cmake 2.8.\r\n\r\nLet's wait for others to review as well. This is very helpful since we have a plan to move this library out of this repository in the future. It is a great start."}, {"author": "damienpontifex", "comment": "I hadn't looked at licensing, so didn't take that into consideration when looking at the OpenSSL change. Maybe it could be behind a define definition for those that want to use it? The first commit is just build changes without this inclusion so this PR could be changed to just include that if that'd work better?\r\n\r\nRe Cmake version: I had just copied the defined version from https://github.com/Azure/azure-storage-fuse/blob/master/azure-storage-cpp-lite/CMakeLists.txt#L1 so that probably also needs changing to match the root CMakeLists.txt if that's what is desired?"}, {"author": "damienpontifex", "comment": "@seguler Added an option to use OpenSSL which defaults to the existing configuration of GnuTLS. Given the default build would be with GnuTLS is this acceptable within the licensing issues you mentioned? Not sure if it covers any inclusion or just the use of it in the final build product."}, {"author": "damienpontifex", "comment": "@seguler any update of feedback on this?"}, {"author": "seguler", "comment": "I think this looks good. OpenSSL license does not allow us to ship binaries that link with OpenSSL. Your PR is good, I think. \r\n\r\n@asorrin-msft and @JasonYang-MSFT for approval"}, {"author": "seguler", "comment": "@damienpontifex - just encountered your fork of tensorflow. Are you going to use this library there ? That'd be good, because this is a library we will support going forward."}, {"author": "damienpontifex", "comment": "@seguler yes, that is the plan. Creating a [Custom Filesystem Plugin](https://www.tensorflow.org/extend/add_filesys) for azure blob storage. See WIP issue I created here https://github.com/tensorflow/tensorflow/issues/18852 \r\n\r\nI had started that work using the curl library. I got it implemented and ironing out use cases until I found this library. Started some work now to remove my custom code and use this library. Much nicer \ud83d\ude04"}, {"author": "damienpontifex", "comment": "@seguler here's the work to use this library for the file system plugin https://github.com/damienpontifex/tensorflow/commits/az-storage-fuse\r\n\r\nCurrently have the bazel dependency pointed to my fork + commit so I can build using TensorFlow's existing dependency on boringssl (i.e. their openssl fork) but wouldn't make the PR back to TensorFlow without changing that to point to this as the main repo, so looking forward to seeing if these changes are accepted."}, {"author": "damienpontifex", "comment": "@asorrin-msft and @JasonYang-MSFT just checking in to see if there's any feedback on this?"}, {"author": "seguler", "comment": "All good - thanks for the contribution @damienpontifex"}]}
{"id": "pr-179", "type": "pr", "title": "Fixing blob deletion failure reporting", "body": "Fixed:\r\n1. wrong logging of local file deletion status\r\n2. fixed blob deletion status check\r\n3. now azs_unlink will report an error if blob deletion actually failed", "author": "AlexanderYukhanov", "created_at": "2018-05-08T05:11:57+00:00", "comments": [{"author": "seguler", "comment": "@rickle-msft can you review and merge ?"}]}
{"id": "pr-166", "type": "pr", "title": "File handle leak fix, debugging improvements", "body": "- Fixing some slow file handle leaks\r\n- Adding some cpp-lite logging statements\r\n- Changing the build to include debug symbols", "author": "asorrin-msft", "created_at": "2018-04-16T17:50:58+00:00", "comments": [{"author": "seguler", "comment": "issue #158 is not fixed - can we include that too ?"}, {"author": "asorrin-msft", "comment": "Sercan - should be done with latest commit, but I haven't tested on Debian.  Worked on Ubuntu 17, although not this exact commit hash, so we should try that again too."}]}
{"id": "pr-155", "type": "pr", "title": "Timeout", "body": "This is a minor change on top of https://github.com/Azure/azure-storage-fuse/pull/146/files.  It sets different timeouts on different operations (5 seconds for most operations, 30 seconds for listing & larger-payload operations.)  Logic for data operations (put blob, put block, get blob) is unchanged.", "author": "asorrin-msft", "created_at": "2018-03-30T20:22:48+00:00", "comments": [{"author": "AlexanderYukhanov", "comment": "are timeout setting autoreset after a request has been completed?"}, {"author": "seguler", "comment": "I am not sure what you mean @AlexanderYukhanov - why would we reset any timeout setting ? Isn't it an absolute number every time ? E.g. 5 second for each get blob properties request."}, {"author": "seguler", "comment": "@asorrin-msft @rickle-msft ; Alex has a good point. This curl handle is being reused all the time... Does the timeout get reset ? For instance, if you have listed blobs in 10 seconds, and then used the same handle for a large file upload. Would it timeout ? Has this been tested ?"}, {"author": "asorrin-msft", "comment": "Latest commit should fix any potential issues by resetting the values to zero (meaning \"no timeout\", according to libcurl docs)"}, {"author": "AlexanderYukhanov", "comment": "LGTM"}, {"author": "seguler", "comment": "lgtm"}]}
{"id": "pr-147", "type": "pr", "title": "Exponential retries", "body": "With a max delay of 60s. All retries and delays included, a single operation should max out at about an hour.", "author": "rickle-msft", "created_at": "2018-03-21T22:18:50+00:00", "comments": [{"author": "seguler", "comment": "lgtm"}]}
{"id": "pr-146", "type": "pr", "title": "Timeouts", "body": "Absolute operation timeouts for requests not expecting a response body. Timeouts based on whether a minimum throughput is maintained throughout the request for requests expecting a response body.", "author": "rickle-msft", "created_at": "2018-03-21T18:53:40+00:00", "comments": [{"author": "seguler", "comment": "Have you tested this ?"}, {"author": "rickle-msft", "comment": "My how the tables have turned!! Using the proxy, I tested the absolute timeouts. There's not a good way for me to let data pass through at a specific rate to test the data-rate timeout, but since it's just a curl option that I implemented following their sample and it's pretty straightforward, I trusted that would work very similarly."}]}
{"id": "pr-136", "type": "pr", "title": "helper copy tool for concurrent data transfers", "body": "We're crazy for doing this; but here is a concurrent copy tool that can be used with blobfuse.\r\n\r\nIn my tests, it improves the upload/download performance by 5X on average through blobfuse.\r\n\r\nHere are a few samples:\r\n1. Copy a directory into another directory\r\n./blobcp -s /home/seguler/mount/images45K/images/ -d /mnt/\r\n\r\n2. Copy a directory of files that matches a pattern (regular file spec)\r\n./blobcp -s /home/seguler/mount/images45K/images/ -d /mnt/ -pattern \"mysamplefile.9*\"", "author": "seguler", "created_at": "2018-03-15T20:45:21+00:00", "comments": [{"author": "seguler", "comment": "If all agrees, I can push another commit that installs blobcp along with blobfuse during the install so that users call blobcp from any where."}, {"author": "AlexanderYukhanov", "comment": "I think you committed the binary by mistake?"}, {"author": "AlexanderYukhanov", "comment": "Do you want to add documentation update for this tool in the same PR?\r\n1. reasoning \r\n2. how to build\r\n3. how to run"}, {"author": "seguler", "comment": "Why not commit the binary as well :) instead of requiring everyone to download Go etc.... Both Go and C is too much for a project.. One has to go!"}, {"author": "asorrin-msft", "comment": "Wait, what?  Why are we committing the binary?  I thought we were going to package the binary in with the blobfuse binaries; you wouldn't have to install go unless you want to build from source"}, {"author": "seguler", "comment": "TODO:\r\n1. Reformat spaces\r\n2. Add README file, modify CMakeLists to install blobcp\r\n\r\n@AlexanderYukhanov does this look good now ?"}, {"author": "AlexanderYukhanov", "comment": "lgtm"}, {"author": "seguler", "comment": "@asorrin-msft I think this is ready to merge now."}]}
{"id": "pr-134", "type": "pr", "title": "Changing blobfuse to log to syslog", "body": "This adds log statements throughout the blobfuse part of the codebase.  Cpplite will be done separately.", "author": "asorrin-msft", "created_at": "2018-03-15T00:41:16+00:00", "comments": [{"author": "seguler", "comment": "Tested and validated on Ubuntu 16.04 with debug log levels."}, {"author": "asorrin-msft", "comment": "Cool, thanks for testing!  On my centos 7.4 machine, I had to configure rsyslog to print debug messages, by default it ignores them.  Do you think we need to give instructions on this?  Or is this obvious enough that we shouldn't worry?"}]}
{"id": "pr-112", "type": "pr", "title": "Fix retry logic for download", "body": "1. Restored state of CurlEasyRequest after initial 503\r\n2. Added handling of curl errors", "author": "AlexanderYukhanov", "created_at": "2018-02-28T11:42:50+00:00", "comments": [{"author": "AlexanderYukhanov", "comment": "Was able to run alexnet against imagenet on 16 NC24r."}, {"author": "seguler", "comment": "You're AWESOME!"}, {"author": "seguler", "comment": "One thing we discussed today earlier:\r\n\r\nThis retry also applies to the curl errors like 'Host not found'.. In this case, fuse client will still try 100 times (every 3 seconds) which will be very painful for simple scenarios like you configure a wrong blobEndpoint... So should we decrease the amount of retries to 10 or something ? Or maybe make it configurable and default to 10 ? @asorrin-msft"}, {"author": "asorrin-msft", "comment": "@seguler - I'm not super concerned about this at the moment.  Before we call fuse_main to actually mount, we do a container_exists() call to validate that we can actually communicate with Storage.  If there's a permanent issue with connecting to Storage, yes this exists call will retry many times before it fails, but at least we won't have actually mounted.  I would assume that most errors coming from libcurl after that are transient.  Of course, we should definitely improve this, but I'm ok with it for now."}, {"author": "AlexanderYukhanov", "comment": "done\r\n\r\nFrom: Sercan Guler [MSFT] <notifications@github.com>\r\nSent: Thursday, March 1, 2018 12:35 PM\r\nTo: Azure/azure-storage-fuse <azure-storage-fuse@noreply.github.com>\r\nCc: Alex Yukhanov <Alexander.Yukhanov@microsoft.com>; Author <author@noreply.github.com>\r\nSubject: Re: [Azure/azure-storage-fuse] Fix retry logic for download (#112)\r\n\r\n\r\n@seguler commented on this pull request.\r\n\r\n________________________________\r\n\r\nIn azure-storage-cpp-lite/src/http/libcurl_http_client.cpp<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2Fazure-storage-fuse%2Fpull%2F112%23discussion_r171685900&data=04%7C01%7CAlexander.Yukhanov%40microsoft.com%7Cd812a1d5cc1a472a25aa08d57fb3e8d6%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636555333045927529%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwifQ%3D%3D%7C-1&sdata=xexL%2BXpNFYc7BYGD61FzpWvYSNEhVea%2Bq1ezWadDqvA%3D&reserved=0>:\r\n\r\n> @@ -51,9 +55,7 @@ namespace microsoft_azure {\r\n\r\n             m_slist = curl_slist_append(m_slist, \"Expect:\");\r\n\r\n             check_code(curl_easy_setopt(m_curl, CURLOPT_HTTPHEADER, m_slist));\r\n\r\n\r\n\r\n-            check_code(curl_easy_perform(m_curl));\r\n\r\n-\r\n\r\n-            return m_code;\r\n\r\n+            return curl_easy_perform(m_curl);\r\n\r\nAlso this needs to target dev branch right ?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2Fazure-storage-fuse%2Fpull%2F112%23discussion_r171685900&data=04%7C01%7CAlexander.Yukhanov%40microsoft.com%7Cd812a1d5cc1a472a25aa08d57fb3e8d6%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636555333045927529%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwifQ%3D%3D%7C-1&sdata=xexL%2BXpNFYc7BYGD61FzpWvYSNEhVea%2Bq1ezWadDqvA%3D&reserved=0>, or mute the thread<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAcZzrTGX9xo_algwS-xoFOPdT3X-G0X6ks5taFt1gaJpZM4SWhKq&data=04%7C01%7CAlexander.Yukhanov%40microsoft.com%7Cd812a1d5cc1a472a25aa08d57fb3e8d6%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636555333045927529%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwifQ%3D%3D%7C-1&sdata=ZDaaGnauu%2FmxKA%2FS571ZGLYbIEjNIzwf9Dz2HsDcJKI%3D&reserved=0>."}]}
{"id": "pr-85", "type": "pr", "title": "Support for non-public clouds", "body": "Issue #22 - Adds support for non-public clouds via the use of account URI from connection.cfg\r\n\r\nHere is connection.cfg template:\r\n```\r\naccountName myaccount\r\naccountKey mykey==\r\ncontainerName mycontainer\r\nblobEndpoint myaccount.blob.core.windows.net\r\n```", "author": "seguler", "created_at": "2018-02-10T07:51:47+00:00", "comments": [{"author": "seguler", "comment": "changed accountUri to blobEndpoint @asorrin"}]}
{"id": "pr-49", "type": "pr", "title": "Fix situation when list blobs return an empty list and continuation token", "body": "It's possible to get an empty list of blobs and continuation token", "author": "AlexanderYukhanov", "created_at": "2018-01-12T19:50:05+00:00", "comments": [{"author": "seguler", "comment": "All tests passed."}, {"author": "AlexanderYukhanov", "comment": "mirobers, you issue has been addressed"}]}
{"id": "pr-41", "type": "pr", "title": "Httpsfix2", "body": "Changing the creation of the azure_blob_client_wrapper to be in azs_init(), instead of main().  This causes curl_global_init() to be called in the correct process, fixing errors we're seeing when using HTTPS in daemon mode.  This replaces a prior PR, https://github.com/Azure/azure-storage-fuse/pull/39 - due to the concern about unintended consequences of doing the NSS initialization in a separate process as making the HTTPS requests.", "author": "asorrin-msft", "created_at": "2017-12-30T08:04:08+00:00", "comments": [{"author": "seguler", "comment": "Validated this on RHEL 7.4.  @MichaelRiss let us know if this fix doesn't work for you"}, {"author": "MichaelRiss", "comment": "Yes, this fix also works on Fedora 26, both as root as well as user.\r\nBut again, I had to cherry-pick commit db6585646b7084f0047883638579d2ca596a3c3c to make the compilation work."}, {"author": "seguler", "comment": "Thanks for testing! We're going to merge all those in a few days, and update the binaries in Linux repos so you can easily install without cloning the repo."}]}
{"id": "pr-36", "type": "pr", "title": "Multithreaded downloading", "body": "1. Now the code will download a blob in multiple threads\r\n2. Added errors handling in blob_client_wrapper::download_blob_to_file\r\n3. Added handling of race condition between reader and blob modification on the server\r\n4. Implemented fisrt chunk downloading for getting blob attributes instead of making an additional get_blob_property code", "author": "AlexanderYukhanov", "created_at": "2017-12-14T20:02:16+00:00", "comments": [{"author": "AlexanderYukhanov", "comment": "Please note, the perf testing is still in progress. Earlier feedback about the approach is welcome -)"}, {"author": "AlexanderYukhanov", "comment": "No, it will not,  it will open existing file if it exists.\r\ni needed to add this option because i have deleted the file creation several lines above\r\n\r\nFrom: Sercan Guler [MSFT] [mailto:notifications@github.com]\r\nSent: Thursday, December 14, 2017 7:13 PM\r\nTo: Azure/azure-storage-fuse <azure-storage-fuse@noreply.github.com>\r\nCc: Alex Yukhanov <Alexander.Yukhanov@microsoft.com>; Author <author@noreply.github.com>\r\nSubject: Re: [Azure/azure-storage-fuse] Multithreaded downloading (#36)\r\n\r\n\r\n@seguler commented on this pull request.\r\n\r\n________________________________\r\n\r\nIn blobfuse/fileapis.cpp<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2Fazure-storage-fuse%2Fpull%2F36%23discussion_r157116400&data=04%7C01%7CAlexander.Yukhanov%40microsoft.com%7Cdd549ae5afd147705fb108d54369b230%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636489043595496191%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwifQ%3D%3D%7C-1&sdata=ymonTJq637x2IdTuqKhUXusPKN4rV78o8%2BRweg8SXNs%3D&reserved=0>:\r\n\r\n>          errno = 0;\r\n\r\n-        int fd = open(mntPath, O_WRONLY);\r\n\r\n+        int fd = open(mntPath, O_CREAT|O_WRONLY, 0770);\r\n\r\nThis would fail if the file was created already - for the attribute cache, or file cache previously. Right ? Why did you add the O_CREAT option ?\r\n\r\n\u2014\r\nYou are receiving this because you authored the thread.\r\nReply to this email directly, view it on GitHub<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2FAzure%2Fazure-storage-fuse%2Fpull%2F36%23pullrequestreview-83707861&data=04%7C01%7CAlexander.Yukhanov%40microsoft.com%7Cdd549ae5afd147705fb108d54369b230%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636489043595496191%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwifQ%3D%3D%7C-1&sdata=7QVpH0Fq%2BXj1L3IjtSCNwvqmOB0%2FvMmPZJffBTOh%2B1c%3D&reserved=0>, or mute the thread<https://na01.safelinks.protection.outlook.com/?url=https%3A%2F%2Fgithub.com%2Fnotifications%2Funsubscribe-auth%2FAcZzrRNCHc-vHUiEfaU0w77EsBQz2IV4ks5tAeOlgaJpZM4RCl2Z&data=04%7C01%7CAlexander.Yukhanov%40microsoft.com%7Cdd549ae5afd147705fb108d54369b230%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C636489043595496191%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwifQ%3D%3D%7C-1&sdata=tN2LuMXI4S7Tnh9NJqCKeT7UDTQibr6g7UHvbABx0g8%3D&reserved=0>."}, {"author": "AlexanderYukhanov", "comment": "Reading a large file small chunk by chunk is super inefficient based on the testings. It seems much more efficient to split the file on number of blocks equal to the number of downloaders and download those large blocks in parallel.\r\nHere are the numbers for downloading 100 GB file (average value of 5 samples)\r\n1. curl 34m5s\r\n2. nfs 2 premium drives: 4m40s\r\n3. prefetcher (256 threads, 16Mb chunks): 2m6s\r\n4. blobfuse singlethreaded: 35m50s\r\n5. blobfuse muthithreaded with 9 threads: 7m50s"}, {"author": "seguler", "comment": "Works on GCC 4.8.2 now. Great job btw - 400MB/s throughput on the download!"}]}
{"id": "pr-2", "type": "pr", "title": "Dev", "body": "Fixed several issues.\r\nAdded rename feature.\r\nAdded CMakeList.txt as a single project.", "author": "JasonYang-MSFT", "created_at": "2017-09-18T10:12:22+00:00", "comments": [{"author": "msftclas", "comment": "@JasonYang-MSFT,\nThanks for your contribution as a Microsoft full-time employee or intern. You do not need to sign a CLA.\n_Thanks,\nMicrosoft Pull Request Bot_"}]}
